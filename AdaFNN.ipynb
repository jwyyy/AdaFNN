{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2440e3d5",
   "metadata": {},
   "source": [
    "# Functional Neural Network with Adaptive Bases\n",
    "\n",
    "In this notewboook, we present a PyTorch implementation of the model proposed in \"Deep Learning for Functional Data Analysis with Adaptive Basis Layers\", ICML 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3017f4",
   "metadata": {},
   "source": [
    "Unlike many functional networks, AdaFNNs take the raw functional data as input and learn to apply parsimonious dimension reduction that focuses only on information relevant to the target rather than irrelevant variation in the input. This operation is done through a noval _Basis Layer_ that consists of _basis nodes_ implemented as micro networks. In addition, the inference and training can be done in an end-to-end manner without preprocessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affceee",
   "metadata": {},
   "source": [
    "# Implementing AdaFNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca2c60",
   "metadata": {},
   "source": [
    "First, we provide the code for two building blocks, a layer normalization module and feedforward network module (with skipping connection). We start by import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07e35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730b770",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "The layer normalization was introduced in [Layer Normalization](https://arxiv.org/abs/1607.06450). It is a transposition of Batch Normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7be4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # d is the normalization dimension\n",
    "        self.d = d\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.randn(d))\n",
    "        self.beta = nn.Parameter(torch.randn(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a torch.Tensor\n",
    "        # avg is the mean value of a layer\n",
    "        avg = x.mean(dim=-1, keepdim=True)\n",
    "        # std is the standard deviation of a layer (eps is added to prevent dividing by zero)\n",
    "        std = x.std(dim=-1, keepdim=True) + self.eps\n",
    "        return (x - avg) / std * self.alpha + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092aa37",
   "metadata": {},
   "source": [
    "Next, we implement a feedforward network module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6e44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, in_d=1, hidden=[4,4,4], dropout=0.1, activation=F.relu):\n",
    "        # in_d      : input dimension, integer\n",
    "        # hidden    : hidden layer dimension, array of integers\n",
    "        # dropout   : dropout probability, a float between 0.0 and 1.0\n",
    "        # activation: activation function at each layer\n",
    "        super().__init__()\n",
    "        self.sigma = activation\n",
    "        dim = [in_d] + hidden + [1]\n",
    "        self.layers = nn.ModuleList([nn.Linear(dim[i-1], dim[i]) for i in range(1, len(dim))])\n",
    "        self.ln = nn.ModuleList([LayerNorm(k) for k in hidden])\n",
    "        self.dp = nn.ModuleList([nn.Dropout(dropout) for _ in range(len(hidden))])\n",
    "\n",
    "    def forward(self, t):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            t = self.layers[i](t)\n",
    "            # skipping connection\n",
    "            t = t + self.ln[i](t)\n",
    "            t = self.sigma(t)\n",
    "            # apply dropout\n",
    "            t = self.dp[i](t)\n",
    "        # linear activation at the last layer\n",
    "        return self.layers[-1](t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58931d84",
   "metadata": {},
   "source": [
    "### Metric operations\n",
    "\n",
    "To build an AdaFNN, we need three new operations: (1) $\\langle f_1, f_2 \\rangle$ (2) $\\| f \\|_2$ and (3) $\\| f \\|_1$. The last two can be established on the first one through: \n",
    "\n",
    "$$ \\| f\\|_2 = \\sqrt{ \\langle f, f \\rangle} $$\n",
    "and \n",
    "$$ \\| f\\|_1 = \\langle 1, |f| \\rangle .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f6d199",
   "metadata": {},
   "source": [
    "Since the input is densely observed (equal spacing is not required), the inner product can be approximated by any numerical integration scheme. Here, we will use the [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2409c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _inner_product(f1, f2, h):\n",
    "    \"\"\"    \n",
    "    f1 - (B, J) : B functions, observed at J time points,\n",
    "    f2 - (B, J) : same as f1\n",
    "    h  - (J-1,1): weights used in the trapezoidal rule\n",
    "    pay attention to dimension\n",
    "    <f1, f2> = sum (h/2) (f1(t{j}) + f2(t{j+1}))\n",
    "    \"\"\"\n",
    "    prod = f1 * f2 # (B, J = len(h) + 1)\n",
    "    return torch.matmul((prod[:, :-1] + prod[:, 1:]), h.unsqueeze(dim=-1))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e8f32",
   "metadata": {},
   "source": [
    "Then $L_1$ and $L_2$ can be easily implememnted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f718d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _l1(f, h):\n",
    "    # f dimension : ( B bases, J )\n",
    "    B, J = f.size()\n",
    "    return _inner_product(torch.abs(f), torch.ones((K, J)), h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43dca4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _l2(f, h):\n",
    "    # f dimension : ( B bases, J )\n",
    "    # output dimension - ( B bases, 1 )\n",
    "    return torch.sqrt(_inner_product(f, f, h)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae9e31",
   "metadata": {},
   "source": [
    "### AdaFNN\n",
    "\n",
    "To prevent the original scale of basis nodes from dominating regularizers, they are normalized.\n",
    "\n",
    "With these in hand, we are ready to present the AdaFNN implmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "223940df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaFNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_base=4, base_hidden=[64, 64, 64], grid=(0, 1),\n",
    "                 sub_hidden=[128, 128, 128], dropout=0.1, lambda1=0.0, lambda2=0.0,\n",
    "                 device=None):\n",
    "        \"\"\"\n",
    "        n_base      : number of basis nodes, integer\n",
    "        base_hidden : hidden layers used in each basis node, array of integers\n",
    "        grid        : observation time grid, array of sorted floats including 0.0 and 1.0\n",
    "        sub_hidden  : hidden layers in the subsequent network, array of integers\n",
    "        dropout     : dropout probability\n",
    "        lambda1     : penalty of L1 regularization, a positive real number\n",
    "        lambda2     : penalty of L2 regularization, a positive real number\n",
    "        device      : device for the training\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_base = n_base\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.device = device\n",
    "        # grid should include both end points\n",
    "        grid = np.array(grid)\n",
    "        # send the time grid tensor to device\n",
    "        self.t = torch.tensor(grid).to(device).float()\n",
    "        self.h = torch.tensor(grid[1:] - grid[:-1]).to(device).float()\n",
    "        # instantiate each basis node in the basis layer\n",
    "        self.BL = nn.ModuleList([FeedForward(1, hidden=base_hidden, dropout=dropout, activation=F.selu)\n",
    "                                 for _ in range(n_base)])\n",
    "        # instantiate the subsequent network\n",
    "        self.FF = FeedForward(n_base, sub_hidden, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, J = x.size()\n",
    "        assert J == self.h.size()[0] + 1\n",
    "        T = self.t.unsqueeze(dim=-1)\n",
    "        # evaluate the current basis nodes at time grid\n",
    "        self.bases = [basis(T).transpose(-1, -2) for basis in self.BL]\n",
    "        \"\"\"\n",
    "        compute each basis node's L2 norm\n",
    "        normalize basis nodes\n",
    "        \"\"\"\n",
    "        l2_norm = _l2(torch.cat(self.bases, dim=0), self.h).detach()\n",
    "        self.normalized_bases = [self.bases[i] / (l2_norm[i, 0] + 1e-6) for i in range(self.n_base)]\n",
    "        # compute each score <basis_i, f> \n",
    "        score = torch.cat([_inner_product(b.repeat((B, 1)), x, self.h) # (B, 1)\n",
    "                           for b in self.bases], dim=-1) # score dim = (B, n_base)\n",
    "        # take the tensor of scores into the subsequent network\n",
    "        out = self.FF(score)\n",
    "        return out\n",
    "\n",
    "    def R1(self, l1_k):\n",
    "        \"\"\"\n",
    "        L1 regularization\n",
    "        l1_k : number of basis nodes to regularize, integer        \n",
    "        \"\"\"\n",
    "        if self.lambda1 == 0: return torch.zeros(1).to(self.device)\n",
    "        # sample l1_k basis nodes to regularize\n",
    "        selected = np.random.choice(self.n_base, min(l1_k, self.n_base), replace=False)\n",
    "        selected_bases = torch.cat([self.normalized_bases[i] for i in selected], dim=0) # (k, J)\n",
    "        return self.lambda1 * torch.mean(_l1(selected_bases, self.h))\n",
    "\n",
    "    def R2(self, l2_pairs):\n",
    "        \"\"\"\n",
    "        L2 regularization\n",
    "        l2_pairs : number of pairs to regularize, integer  \n",
    "        \"\"\"\n",
    "        if self.lambda2 == 0 or self.n_base == 1: return torch.zeros(1).to(self.device)\n",
    "        k = min(l2_pairs, self.n_base * (self.n_base - 1) // 2)\n",
    "        f1, f2 = [None] * k, [None] * k\n",
    "        for i in range(k):\n",
    "            a, b = np.random.choice(self.n_base, 2, replace=False)\n",
    "            f1[i], f2[i] = self.normalized_bases[a], self.normalized_bases[b]\n",
    "        return self.lambda2 * torch.mean(torch.abs(_inner_product(torch.cat(f1, dim=0),\n",
    "                                                                  torch.cat(f2, dim=0),\n",
    "                                                                  self.h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d739",
   "metadata": {},
   "source": [
    "#  Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e652727",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "\n",
    "Data is generated based on the following model:\n",
    "\n",
    "$$ X(t) \\ = \\ \\sum_{k=1}^{50} c_k \\phi_k (t), \\quad t \\in [0,1] ,$$ \n",
    "where terms on the right hand side are defined as:\n",
    "\n",
    "1. $\\phi_1 (t) = 1$ and $ \\phi_k (t) = \\sqrt{2} \\cos ( (k-1) \\pi t)$ for $k = 2, \\dots, 50$.\n",
    "2. $c_k = z_k r_k$, and $r_k$ are i.i.d. uniform random variables on $[-\\sqrt{3}, \\sqrt{3}]$.\n",
    "\n",
    "Case 1: $z_1 = 20$, $z_2 = z_3 = 5$, and $z_k = 1$ for $k \\geq 4$. $y = \\big( \\langle \\phi_3, X \\rangle \\big)^2$.\n",
    "\n",
    "Case 2 and 3: $z_1 = z_3 = 5$, $z_5 = z_{10} = 3$, and $z_k = 1$ for other $k$. $y = \\big( \\langle \\phi_5, X \\rangle \\big)^2$.\n",
    "\n",
    "Case 4: $X$ has the same configurations as Case 2. But $y=\\langle \\beta_2, X \\rangle + \\big( \\langle \\beta_1, X \\rangle \\big)^2$ with\n",
    "\n",
    "$$ \\beta_1 (t) = (4 - 16t) \\cdot 1 \\big\\{ 0 \\leq t \\leq 1/4 \\big\\} $$\n",
    "and\n",
    "$$ \\beta_2 (t) = \\big( 4 - 16|t-1/2| \\big) \\cdot 1 \\big\\{ |t-1/2| \\leq 1/4 \\big\\} .$$\n",
    "\n",
    "For each time point $t$, the observed $X(t)$ may be contaminated by measurement error, i.e.\n",
    "\n",
    "$$ \\tilde{X} (t) = X (t) + \\eta_t, \\quad \\eta_t \\stackrel{i.i.d.}{\\sim} N (0, \\sigma^2_1) .$$\n",
    "\n",
    "The response $y$ may also have noise, i.e. $\\tilde{y} = y + \\epsilon$ where $\\epsilon \\stackrel{i.i.d.}{\\sim} N (0, \\sigma^2_2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143e254",
   "metadata": {},
   "source": [
    "First, we import necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575c8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be7c3b",
   "metadata": {},
   "source": [
    "Next, we list configurations for each cases and implememnt functions for generating $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5009dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = [20, 5, 5] + [1] * 47\n",
    "z2 = [1] * 50\n",
    "z2[0] = z2[2] = 5\n",
    "z2[4] = z2[9] = 3\n",
    "Z = [z1, z2, z2, [1] * 50]\n",
    "\n",
    "\n",
    "def _phi(k):\n",
    "    if k == 1: return lambda t: np.ones((len(t),))\n",
    "    return lambda t : np.sqrt(2) * np.cos((k-1) * np.pi * t)\n",
    "\n",
    "\n",
    "def _b1(t):\n",
    "    return (4 - 16 * t) * (0 <= t) * (t <= 1/4)\n",
    "\n",
    "\n",
    "def _b2(t):\n",
    "    return (4 - 16 * np.abs(1/2 - t)) * (1/4 <= t) * (t <= 3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d3a4e",
   "metadata": {},
   "source": [
    "The DataGenerator class generates data and save it to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb165a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "\n",
    "    def __init__(self, grid, case=1, me=1, err=1):\n",
    "        \"\"\"\n",
    "        grid : array of time points, floats\n",
    "        case : case number, integer\n",
    "        me   : variance of measurement error added to X, non-negative real value\n",
    "        err  : variance of noise added to Y, non-negative real value\n",
    "        \"\"\"\n",
    "        self.t = np.array(grid)\n",
    "        # measurement error\n",
    "        self.me = me\n",
    "        self.err = err\n",
    "        # case - 1\n",
    "        self.case = case\n",
    "        self.z = np.array(Z[case-1])\n",
    "\n",
    "    def generate(self, n=1000):\n",
    "        \"\"\"\n",
    "        n : number of subjects to generate, integer\n",
    "        \"\"\"\n",
    "        # X = sum c_k phi_k\n",
    "        # c_k = z_k r_k, r_k iid unif[-sqrt(3), sqrt(3)]\n",
    "        # generate r\n",
    "        r = np.random.uniform(low=-np.sqrt(3), high=np.sqrt(3), size=(n, 50))\n",
    "        c = r * self.z # (n, 50) elementwise multiplication\n",
    "        phi = np.array([_phi(k)(self.t) for k in range(1, 51)]) # (50, len(self.t))\n",
    "        X = np.matmul(c, phi) # (n, len(self.t))\n",
    "        Y = np.zeros((n, 1))\n",
    "        if self.case == 1:\n",
    "            Y = (c[:, 2]) ** 2\n",
    "        elif self.case == 4:\n",
    "            beta1 = _b1(self.t)\n",
    "            beta2 = _b2(self.t)\n",
    "            h = np.array(self.t[1:] - self.t[:-1]).T\n",
    "            for i in range(n):\n",
    "                Y[i, 0] = self._inner_product(beta2, X[i, :], h) + _inner_product(beta1, X[i, :], h) ** 2\n",
    "\n",
    "        else: # self.case = 2 or 3\n",
    "            Y = (c[:, 4]) ** 2        \n",
    "        self.X = X + np.random.normal(0, self.me, size=(n, len(self.t)))\n",
    "        self.Y = Y.reshape((n, 1)) + np.random.normal(0, self.err, size=(n, 1))\n",
    "        \n",
    "    def _inner_product(self, f1, f2, h):\n",
    "        prod = f1 * f2\n",
    "        if len(prod.shape) < 2:\n",
    "            prod = prod.reshape((1, -1))\n",
    "        res = np.matmul(prod[:, :-1] + prod[:, 1:], h) / 2\n",
    "        return res\n",
    "\n",
    "    def save(self, folder):\n",
    "        \"\"\"\n",
    "        folder : folder where observations are saved\n",
    "        \"\"\"\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "        X_df = pd.DataFrame(self.X)\n",
    "        Y_df = pd.DataFrame(self.Y)\n",
    "        T_df = pd.DataFrame(self.t.reshape((1, -1)))\n",
    "        X_df.to_csv(folder + \"X.csv\", index=False, header=None)\n",
    "        Y_df.to_csv(folder + \"Y.csv\", index=False, header=None)\n",
    "        T_df.to_csv(folder + \"T.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720e239",
   "metadata": {},
   "source": [
    "The time grid doesn't have to be equally spaced. The model works as long as the time gap is small enough for numerical integration to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98deebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid(d=0.02):\n",
    "    \"\"\"\n",
    "    d : maximum time gap between two consecutive time points, float\n",
    "    \"\"\"\n",
    "    grid = [0.0]\n",
    "    while 1.0 - grid[-1] > d:\n",
    "        grid.append(grid[-1] + np.random.uniform(0, d, 1).item())\n",
    "    return grid + [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58325a",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "This module reads the dataset from csv files and split it according to a pre-specific train/valid/test ratio. The dataset is standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3d8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, batch_size, X, Y, T, split=(8, 1, 1), random_seed=10294):        \n",
    "        \"\"\"\n",
    "        batch_size : batch size, integer\n",
    "        X - (n, J) : pandas.DataFrame for observed functional data, n - subject number, J - number of time points\n",
    "        Y - (n, 1) : pandas.DataFrame for response\n",
    "        split      : train/valid/test split\n",
    "        random_seed: random seed for training data re-shuffle\n",
    "        \"\"\"        \n",
    "        self.n, J = X.shape\n",
    "        self.t = T.iloc[0, :].to_numpy()\n",
    "        X, Y = X.values, Y.values\n",
    "\n",
    "        # train/valid/test split\n",
    "        self.batch_size = batch_size\n",
    "        train_n = self.n // sum(split) * split[0]\n",
    "        valid_n = self.n // sum(split) * split[1]\n",
    "        test_n = self.n - train_n - valid_n\n",
    "        self.train_B = train_n // batch_size\n",
    "        self.valid_B = valid_n // batch_size\n",
    "        self.test_B = test_n // batch_size\n",
    "\n",
    "        # random shuffle\n",
    "        np.random.seed(random_seed)\n",
    "        _order = list(range(self.n))\n",
    "        np.random.shuffle(_order)\n",
    "        X = X[_order, :]\n",
    "        Y = Y[_order, :]\n",
    "\n",
    "        # standardize dataset based on the training dataset\n",
    "        X_standardizer = StandardScaler()\n",
    "        Y_standardizer = StandardScaler()\n",
    "\n",
    "        # train/valid/test split\n",
    "        self.train_X = X[:(self.train_B * self.batch_size), :]\n",
    "        self.train_Y = Y[:(self.train_B * self.batch_size), :]\n",
    "        X_standardizer.fit(self.train_X)\n",
    "        Y_standardizer.fit(self.train_Y)\n",
    "        self.train_X = X_standardizer.transform(self.train_X)\n",
    "        self.train_Y = Y_standardizer.transform(self.train_Y)\n",
    "\n",
    "        self.valid_X = X[(self.train_B * self.batch_size):((self.train_B + self.valid_B) * self.batch_size), :]\n",
    "        self.valid_Y = Y[(self.train_B * self.batch_size):((self.train_B + self.valid_B) * self.batch_size), :]\n",
    "        self.valid_X = X_standardizer.transform(self.valid_X)\n",
    "        self.valid_Y = Y_standardizer.transform(self.valid_Y)\n",
    "\n",
    "        self.test_X = X[((self.train_B + self.valid_B) * self.batch_size):, :]\n",
    "        self.test_Y = Y[((self.train_B + self.valid_B) * self.batch_size):, :]\n",
    "        self.test_X = X_standardizer.transform(self.test_X)\n",
    "        self.test_Y = Y_standardizer.transform(self.test_Y)\n",
    "\n",
    "    def shuffle(self):\n",
    "        # re-shuffle the training dataset\n",
    "        train_size = self.train_X.shape[0]\n",
    "        new_order = list(range(train_size))\n",
    "        np.random.shuffle(new_order)\n",
    "        self.train_X = self.train_X[new_order, :]\n",
    "        self.train_Y = self.train_Y[new_order, :]\n",
    "\n",
    "    def _batch_generator(self, X, Y, N):\n",
    "\n",
    "        def generator_func():\n",
    "            for i in range(1, N):\n",
    "                x = X[((i - 1) * self.batch_size):((i) * self.batch_size), :]\n",
    "                y = Y[((i - 1) * self.batch_size):((i) * self.batch_size), :]\n",
    "\n",
    "                yield torch.Tensor(x), torch.Tensor(y)\n",
    "\n",
    "        return generator_func()\n",
    "\n",
    "    def get_train_batch(self):\n",
    "        return self._batch_generator(self.train_X, self.train_Y, self.train_B)\n",
    "\n",
    "    def get_valid_batch(self):\n",
    "        return self._batch_generator(self.valid_X, self.valid_Y, self.valid_B)\n",
    "\n",
    "    def get_test_batch(self):\n",
    "        return self._batch_generator(self.test_X, self.test_Y, self.test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49638f",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ba9f4",
   "metadata": {},
   "source": [
    "First, we load necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9040ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaef536",
   "metadata": {},
   "source": [
    "A dataset will be generated if it is not present. \n",
    "\n",
    "Here, we set the measurement error variance to be 1 and noise variance to be 0.2.\n",
    "\n",
    "**Note**: in this example, we use a flexible time point gap (**not** equal spacing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a6f7cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data\").is_dir():\n",
    "    d = 0.02\n",
    "    # tp = np.arange(0, 1 + d, d)\n",
    "    tp = random_grid(d)\n",
    "    DatGen = DataGenerator(tp, case=3, me=1.0, err=0.2)\n",
    "    DatGen.generate(4000)\n",
    "    DatGen.save(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac248ade",
   "metadata": {},
   "source": [
    "The dataset is loaded and split for training/validation/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d177bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "split = (64, 16, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9c71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"data/X.csv\", header=None)\n",
    "Y = pd.read_csv(\"data/Y.csv\", header=None)\n",
    "T = pd.read_csv(\"data/T.csv\", header=None)\n",
    "grid = T.iloc[0, :].to_list()\n",
    "dataLoader = DataLoader(batch_size,  X, Y, T, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d09fa1",
   "metadata": {},
   "source": [
    "Prepare the model and other training configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a0848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up CPU/GPU\n",
    "device = torch.device(\"cpu\") \n",
    "# model configuration\n",
    "base_hidden = [256, 256, 256, 256]\n",
    "sub_hidden = [64, 64]\n",
    "n_base = 2\n",
    "lambda1, l1_k = 0.0, 2\n",
    "lambda2, l2_pairs = 0.0, 3\n",
    "split = (64, 16, 20)\n",
    "dropout = 0.1\n",
    "save_model_every = 100\n",
    "model = AdaFNN(n_base=n_base,\n",
    "               base_hidden=base_hidden,\n",
    "               grid=grid,\n",
    "               sub_hidden=sub_hidden,\n",
    "               dropout=dropout,\n",
    "               lambda1=lambda1,\n",
    "               lambda2=lambda2,\n",
    "               device=device)\n",
    "# send model to CPU/GPU\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf7301ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training configuration\n",
    "epoch = 500\n",
    "pred_loss_train_history = []\n",
    "total_loss_train_history = []\n",
    "loss_valid_history = []\n",
    "# instantiate an optimizer\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "# use MSE loss\n",
    "compute_loss = torch.nn.MSELoss()\n",
    "min_valid_loss = sys.maxsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cc291",
   "metadata": {},
   "source": [
    "Create a folder to save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593bd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"train/\"\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e610e",
   "metadata": {},
   "source": [
    "Save and load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28319be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(folder, k, n_base, base_hidden, grid, sub_hidden, dropout, lambda1, lambda2, model, optimizer):\n",
    "    checkpoint = {'n_base': n_base,\n",
    "                  'base_hidden': base_hidden,\n",
    "                  'grid': grid,\n",
    "                  'sub_hidden': sub_hidden,\n",
    "                  'dropout': dropout,\n",
    "                  'lambda1' : lambda1,\n",
    "                  'lambda2' : lambda2,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, folder + str(k) + '_' + 'checkpoint.pth')\n",
    "\n",
    "\n",
    "def load_model(file_path, device):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model = AdaFNN(n_base=checkpoint['n_base'],\n",
    "                   base_hidden=checkpoint['base_hidden'],\n",
    "                   grid=checkpoint['grid'],\n",
    "                   sub_hidden=checkpoint['sub_hidden'],\n",
    "                   dropout=checkpoint['dropout'],\n",
    "                   lambda1=checkpoint['lambda1'],\n",
    "                   lambda2=checkpoint['lambda2'],\n",
    "                   device=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    _ = model.to(device)\n",
    "    return model, checkpoint['grid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ef584",
   "metadata": {},
   "source": [
    "Training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "235a85eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training mini-batch : 0 total loss = 1.0388671159744263\n",
      "epoch: 1 \n",
      " prediction training loss =  1.0485073422130786 validation loss =  1.0436294823884964\n",
      "training mini-batch : 0 total loss = 1.0707967281341553\n",
      "epoch: 2 \n",
      " prediction training loss =  1.028722700319792 validation loss =  1.0309243500232697\n",
      "training mini-batch : 0 total loss = 0.9236045479774475\n",
      "epoch: 3 \n",
      " prediction training loss =  1.0342886416535628 validation loss =  1.036406546831131\n",
      "training mini-batch : 0 total loss = 1.0610398054122925\n",
      "epoch: 4 \n",
      " prediction training loss =  1.0321995051283586 validation loss =  1.0277564525604248\n",
      "training mini-batch : 0 total loss = 0.9687504768371582\n",
      "epoch: 5 \n",
      " prediction training loss =  1.0209797997223704 validation loss =  1.0325876772403717\n",
      "training mini-batch : 0 total loss = 1.0729657411575317\n",
      "epoch: 6 \n",
      " prediction training loss =  1.0076316249997992 validation loss =  1.0363144725561142\n",
      "training mini-batch : 0 total loss = 0.9780945181846619\n",
      "epoch: 7 \n",
      " prediction training loss =  1.0154353693911904 validation loss =  1.0337424874305725\n",
      "training mini-batch : 0 total loss = 0.9989498257637024\n",
      "epoch: 8 \n",
      " prediction training loss =  1.0246777126663609 validation loss =  1.0293316543102264\n",
      "training mini-batch : 0 total loss = 0.9535146951675415\n",
      "epoch: 9 \n",
      " prediction training loss =  1.018644674828178 validation loss =  1.0314561873674393\n",
      "training mini-batch : 0 total loss = 1.118795394897461\n",
      "epoch: 10 \n",
      " prediction training loss =  1.0274252860169661 validation loss =  1.033486470580101\n",
      "training mini-batch : 0 total loss = 0.9369344711303711\n",
      "epoch: 11 \n",
      " prediction training loss =  1.0145825304483111 validation loss =  1.031092181801796\n",
      "training mini-batch : 0 total loss = 0.9469602108001709\n",
      "epoch: 12 \n",
      " prediction training loss =  1.0128825589230186 validation loss =  1.0331896394491196\n",
      "training mini-batch : 0 total loss = 0.897688627243042\n",
      "epoch: 13 \n",
      " prediction training loss =  1.0201479039694135 validation loss =  1.0324396640062332\n",
      "training mini-batch : 0 total loss = 0.8427876234054565\n",
      "epoch: 14 \n",
      " prediction training loss =  1.0168517639762478 validation loss =  1.03841932117939\n",
      "training mini-batch : 0 total loss = 0.9429373741149902\n",
      "epoch: 15 \n",
      " prediction training loss =  1.010891594384846 validation loss =  1.0356996953487396\n",
      "training mini-batch : 0 total loss = 0.9251701235771179\n",
      "epoch: 16 \n",
      " prediction training loss =  1.0052876629327472 validation loss =  1.0328634083271027\n",
      "training mini-batch : 0 total loss = 1.0638670921325684\n",
      "epoch: 17 \n",
      " prediction training loss =  1.0026840159767552 validation loss =  1.0334937125444412\n",
      "training mini-batch : 0 total loss = 0.8193159699440002\n",
      "epoch: 18 \n",
      " prediction training loss =  1.0277764201164246 validation loss =  1.0383211970329285\n",
      "training mini-batch : 0 total loss = 1.147721290588379\n",
      "epoch: 19 \n",
      " prediction training loss =  1.013763804184763 validation loss =  1.0329961776733398\n",
      "training mini-batch : 0 total loss = 1.094694972038269\n",
      "epoch: 20 \n",
      " prediction training loss =  1.014927835840928 validation loss =  1.0350087136030197\n",
      "training mini-batch : 0 total loss = 1.0280200242996216\n",
      "epoch: 21 \n",
      " prediction training loss =  1.0094670402376276 validation loss =  1.0314815044403076\n",
      "training mini-batch : 0 total loss = 0.9775499701499939\n",
      "epoch: 22 \n",
      " prediction training loss =  1.0005583982718618 validation loss =  1.0324861705303192\n",
      "training mini-batch : 0 total loss = 1.0412709712982178\n",
      "epoch: 23 \n",
      " prediction training loss =  0.9976388059164348 validation loss =  1.0337344259023666\n",
      "training mini-batch : 0 total loss = 1.0228698253631592\n",
      "epoch: 24 \n",
      " prediction training loss =  1.012737722773301 validation loss =  1.0338603556156158\n",
      "training mini-batch : 0 total loss = 0.9022022485733032\n",
      "epoch: 25 \n",
      " prediction training loss =  1.0062522731329266 validation loss =  1.0342018008232117\n",
      "training mini-batch : 0 total loss = 0.8495292067527771\n",
      "epoch: 26 \n",
      " prediction training loss =  1.002885981609947 validation loss =  1.02952241897583\n",
      "training mini-batch : 0 total loss = 1.1110001802444458\n",
      "epoch: 27 \n",
      " prediction training loss =  1.0078261118186147 validation loss =  1.030730128288269\n",
      "training mini-batch : 0 total loss = 1.1396926641464233\n",
      "epoch: 28 \n",
      " prediction training loss =  1.0117951380579095 validation loss =  1.0316045731306076\n",
      "training mini-batch : 0 total loss = 1.1488662958145142\n",
      "epoch: 29 \n",
      " prediction training loss =  1.0035601666099148 validation loss =  1.0329186022281647\n",
      "training mini-batch : 0 total loss = 0.9268279671669006\n",
      "epoch: 30 \n",
      " prediction training loss =  1.0102080671410811 validation loss =  1.031623974442482\n",
      "training mini-batch : 0 total loss = 0.9786784648895264\n",
      "epoch: 31 \n",
      " prediction training loss =  1.0034652885637785 validation loss =  1.0358600616455078\n",
      "training mini-batch : 0 total loss = 1.026528000831604\n",
      "epoch: 32 \n",
      " prediction training loss =  0.998850637360623 validation loss =  1.0330617278814316\n",
      "training mini-batch : 0 total loss = 0.8541063070297241\n",
      "epoch: 33 \n",
      " prediction training loss =  1.0068269058277732 validation loss =  1.0310131907463074\n",
      "training mini-batch : 0 total loss = 0.918147087097168\n",
      "epoch: 34 \n",
      " prediction training loss =  0.9957678223911085 validation loss =  1.0312284529209137\n",
      "training mini-batch : 0 total loss = 1.0474272966384888\n",
      "epoch: 35 \n",
      " prediction training loss =  1.0028477405246936 validation loss =  1.0307484418153763\n",
      "training mini-batch : 0 total loss = 1.0604190826416016\n",
      "epoch: 36 \n",
      " prediction training loss =  1.002697395650964 validation loss =  1.030385434627533\n",
      "training mini-batch : 0 total loss = 0.9619292616844177\n",
      "epoch: 37 \n",
      " prediction training loss =  1.0099152985372042 validation loss =  1.0314721167087555\n",
      "training mini-batch : 0 total loss = 0.9187803864479065\n",
      "epoch: 38 \n",
      " prediction training loss =  1.0024616655550505 validation loss =  1.0317916423082352\n",
      "training mini-batch : 0 total loss = 0.9466882944107056\n",
      "epoch: 39 \n",
      " prediction training loss =  1.003297667754324 validation loss =  1.032239481806755\n",
      "training mini-batch : 0 total loss = 1.0295264720916748\n",
      "epoch: 40 \n",
      " prediction training loss =  1.003650492743442 validation loss =  1.0333401411771774\n",
      "training mini-batch : 0 total loss = 0.9067543148994446\n",
      "epoch: 41 \n",
      " prediction training loss =  1.000324785709381 validation loss =  1.0317592024803162\n",
      "training mini-batch : 0 total loss = 1.1328967809677124\n",
      "epoch: 42 \n",
      " prediction training loss =  1.0019631228948895 validation loss =  1.0315591394901276\n",
      "training mini-batch : 0 total loss = 0.9998350739479065\n",
      "epoch: 43 \n",
      " prediction training loss =  0.9969165262423063 validation loss =  1.0320380628108978\n",
      "training mini-batch : 0 total loss = 0.8649594187736511\n",
      "epoch: 44 \n",
      " prediction training loss =  1.0044266801131398 validation loss =  1.0316528528928757\n",
      "training mini-batch : 0 total loss = 0.9734931588172913\n",
      "epoch: 45 \n",
      " prediction training loss =  1.0088222183679278 validation loss =  1.031538873910904\n",
      "training mini-batch : 0 total loss = 1.1824750900268555\n",
      "epoch: 46 \n",
      " prediction training loss =  1.0073660016059875 validation loss =  1.0321823507547379\n",
      "training mini-batch : 0 total loss = 1.0222266912460327\n",
      "epoch: 47 \n",
      " prediction training loss =  0.9987182648558366 validation loss =  1.030468612909317\n",
      "training mini-batch : 0 total loss = 0.9391666650772095\n",
      "epoch: 48 \n",
      " prediction training loss =  1.005072571729359 validation loss =  1.0294248014688492\n",
      "training mini-batch : 0 total loss = 0.951056718826294\n",
      "epoch: 49 \n",
      " prediction training loss =  1.0063878391918384 validation loss =  1.0294775068759918\n",
      "training mini-batch : 0 total loss = 1.0145231485366821\n",
      "epoch: 50 \n",
      " prediction training loss =  1.0077895365263287 validation loss =  1.032065823674202\n",
      "training mini-batch : 0 total loss = 0.9841028451919556\n",
      "epoch: 51 \n",
      " prediction training loss =  1.0058405587547703 validation loss =  1.0287403464317322\n",
      "training mini-batch : 0 total loss = 0.9339876174926758\n",
      "epoch: 52 \n",
      " prediction training loss =  1.0134530443894236 validation loss =  1.0277024954557419\n",
      "training mini-batch : 0 total loss = 1.001446008682251\n",
      "epoch: 53 \n",
      " prediction training loss =  1.0012244833143134 validation loss =  1.0306050777435303\n",
      "training mini-batch : 0 total loss = 0.8977040648460388\n",
      "epoch: 54 \n",
      " prediction training loss =  1.0051141851826717 validation loss =  1.0283195078372955\n",
      "training mini-batch : 0 total loss = 1.1187939643859863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 55 \n",
      " prediction training loss =  1.0052233087389093 validation loss =  1.029231384396553\n",
      "training mini-batch : 0 total loss = 0.9029016494750977\n",
      "epoch: 56 \n",
      " prediction training loss =  1.0057726220080727 validation loss =  1.0280882716178894\n",
      "training mini-batch : 0 total loss = 1.0548726320266724\n",
      "epoch: 57 \n",
      " prediction training loss =  0.9998572437386763 validation loss =  1.0290807336568832\n",
      "training mini-batch : 0 total loss = 1.075960397720337\n",
      "epoch: 58 \n",
      " prediction training loss =  1.0109677753950421 validation loss =  1.030570849776268\n",
      "training mini-batch : 0 total loss = 0.9724870920181274\n",
      "epoch: 59 \n",
      " prediction training loss =  0.99587229992214 validation loss =  1.028502196073532\n",
      "training mini-batch : 0 total loss = 1.1129528284072876\n",
      "epoch: 60 \n",
      " prediction training loss =  0.9958246726738779 validation loss =  1.0303821563720703\n",
      "training mini-batch : 0 total loss = 1.0077675580978394\n",
      "epoch: 61 \n",
      " prediction training loss =  1.0001644429407621 validation loss =  1.0299439579248428\n",
      "training mini-batch : 0 total loss = 1.0245567560195923\n",
      "epoch: 62 \n",
      " prediction training loss =  1.000642428272649 validation loss =  1.0318873673677444\n",
      "training mini-batch : 0 total loss = 0.9954568147659302\n",
      "epoch: 63 \n",
      " prediction training loss =  0.9943770954483434 validation loss =  1.0310202091932297\n",
      "training mini-batch : 0 total loss = 0.9082739353179932\n",
      "epoch: 64 \n",
      " prediction training loss =  0.9945269132915296 validation loss =  1.0303856432437897\n",
      "training mini-batch : 0 total loss = 1.0496786832809448\n",
      "epoch: 65 \n",
      " prediction training loss =  0.9930739434141862 validation loss =  1.0312047004699707\n",
      "training mini-batch : 0 total loss = 0.981330931186676\n",
      "epoch: 66 \n",
      " prediction training loss =  0.9964979385074816 validation loss =  1.0294205099344254\n",
      "training mini-batch : 0 total loss = 1.045881986618042\n",
      "epoch: 67 \n",
      " prediction training loss =  1.007058372623042 validation loss =  1.0300597250461578\n",
      "training mini-batch : 0 total loss = 0.9852677583694458\n",
      "epoch: 68 \n",
      " prediction training loss =  1.0009122835962396 validation loss =  1.0331209897994995\n",
      "training mini-batch : 0 total loss = 1.1497831344604492\n",
      "epoch: 69 \n",
      " prediction training loss =  1.0053787702008297 validation loss =  1.033019244670868\n",
      "training mini-batch : 0 total loss = 0.9087949991226196\n",
      "epoch: 70 \n",
      " prediction training loss =  0.996654024249629 validation loss =  1.0318748950958252\n",
      "training mini-batch : 0 total loss = 1.0587202310562134\n",
      "epoch: 71 \n",
      " prediction training loss =  1.0024145025956004 validation loss =  1.0321712344884872\n",
      "training mini-batch : 0 total loss = 1.0777462720870972\n",
      "epoch: 72 \n",
      " prediction training loss =  1.0018968927232843 validation loss =  1.0322010964155197\n",
      "training mini-batch : 0 total loss = 1.1251333951950073\n",
      "epoch: 73 \n",
      " prediction training loss =  1.0020301279268766 validation loss =  1.030976578593254\n",
      "training mini-batch : 0 total loss = 1.0634310245513916\n",
      "epoch: 74 \n",
      " prediction training loss =  1.0044755308251632 validation loss =  1.0312355011701584\n",
      "training mini-batch : 0 total loss = 0.8380366563796997\n",
      "epoch: 75 \n",
      " prediction training loss =  1.0028763507541858 validation loss =  1.0307738929986954\n",
      "training mini-batch : 0 total loss = 1.048110008239746\n",
      "epoch: 76 \n",
      " prediction training loss =  1.0089130338869596 validation loss =  1.0317762047052383\n",
      "training mini-batch : 0 total loss = 0.9414457082748413\n",
      "epoch: 77 \n",
      " prediction training loss =  1.0026190688735561 validation loss =  1.031955063343048\n",
      "training mini-batch : 0 total loss = 1.1795122623443604\n",
      "epoch: 78 \n",
      " prediction training loss =  0.9983969989575838 validation loss =  1.0307796895503998\n",
      "training mini-batch : 0 total loss = 0.9117798209190369\n",
      "epoch: 79 \n",
      " prediction training loss =  1.0000166422442387 validation loss =  1.0302905142307281\n",
      "training mini-batch : 0 total loss = 1.2225288152694702\n",
      "epoch: 80 \n",
      " prediction training loss =  0.9999736453357496 validation loss =  1.031320869922638\n",
      "training mini-batch : 0 total loss = 0.9396008849143982\n",
      "epoch: 81 \n",
      " prediction training loss =  0.9966558249373185 validation loss =  1.0303695499897003\n",
      "training mini-batch : 0 total loss = 1.0733606815338135\n",
      "epoch: 82 \n",
      " prediction training loss =  1.0046500438138057 validation loss =  1.0317699164152145\n",
      "training mini-batch : 0 total loss = 1.067939281463623\n",
      "epoch: 83 \n",
      " prediction training loss =  1.0075507258114063 validation loss =  1.0317901819944382\n",
      "training mini-batch : 0 total loss = 0.9964972734451294\n",
      "epoch: 84 \n",
      " prediction training loss =  0.9990683950875935 validation loss =  1.0308965891599655\n",
      "training mini-batch : 0 total loss = 1.0874515771865845\n",
      "epoch: 85 \n",
      " prediction training loss =  1.0022723361065513 validation loss =  1.031343549489975\n",
      "training mini-batch : 0 total loss = 1.0136545896530151\n",
      "epoch: 86 \n",
      " prediction training loss =  1.0065616118280512 validation loss =  1.032223954796791\n",
      "training mini-batch : 0 total loss = 1.135783076286316\n",
      "epoch: 87 \n",
      " prediction training loss =  0.9999315989644904 validation loss =  1.0296163707971573\n",
      "training mini-batch : 0 total loss = 1.0405765771865845\n",
      "epoch: 88 \n",
      " prediction training loss =  1.0122293108387996 validation loss =  1.0301597118377686\n",
      "training mini-batch : 0 total loss = 1.0186744928359985\n",
      "epoch: 89 \n",
      " prediction training loss =  1.004334829355541 validation loss =  1.032520905137062\n",
      "training mini-batch : 0 total loss = 0.9005296230316162\n",
      "epoch: 90 \n",
      " prediction training loss =  0.9945601475866217 validation loss =  1.0324940383434296\n",
      "training mini-batch : 0 total loss = 0.9850575923919678\n",
      "epoch: 91 \n",
      " prediction training loss =  1.0042293793276738 validation loss =  1.0330951809883118\n",
      "training mini-batch : 0 total loss = 1.0250048637390137\n",
      "epoch: 92 \n",
      " prediction training loss =  1.0103472471237183 validation loss =  1.031676396727562\n",
      "training mini-batch : 0 total loss = 1.042609453201294\n",
      "epoch: 93 \n",
      " prediction training loss =  1.0052251376603778 validation loss =  1.0302262157201767\n",
      "training mini-batch : 0 total loss = 0.9214058518409729\n",
      "epoch: 94 \n",
      " prediction training loss =  1.0030688925793296 validation loss =  1.0304809510707855\n",
      "training mini-batch : 0 total loss = 0.9691546559333801\n",
      "epoch: 95 \n",
      " prediction training loss =  0.9980466585410269 validation loss =  1.0313300490379333\n",
      "training mini-batch : 0 total loss = 0.9725520014762878\n",
      "epoch: 96 \n",
      " prediction training loss =  0.9922106077796534 validation loss =  1.0306176096200943\n",
      "training mini-batch : 0 total loss = 1.0044397115707397\n",
      "epoch: 97 \n",
      " prediction training loss =  1.0025415326419629 validation loss =  1.0310417264699936\n",
      "training mini-batch : 0 total loss = 1.055905818939209\n",
      "epoch: 98 \n",
      " prediction training loss =  1.0034998153385364 validation loss =  1.0307701230049133\n",
      "training mini-batch : 0 total loss = 0.8777312636375427\n",
      "epoch: 99 \n",
      " prediction training loss =  0.999268042413812 validation loss =  1.0296323150396347\n",
      "training mini-batch : 0 total loss = 0.8960732221603394\n",
      "epoch: 100 \n",
      " prediction training loss =  1.0054616300683272 validation loss =  1.028862103819847\n",
      "training mini-batch : 0 total loss = 0.9854748249053955\n",
      "epoch: 101 \n",
      " prediction training loss =  0.9976270543901544 validation loss =  1.0293554216623306\n",
      "training mini-batch : 0 total loss = 1.020758032798767\n",
      "epoch: 102 \n",
      " prediction training loss =  1.0002975840317576 validation loss =  1.0294310301542282\n",
      "training mini-batch : 0 total loss = 1.0265960693359375\n",
      "epoch: 103 \n",
      " prediction training loss =  0.9969144181201333 validation loss =  1.0293533354997635\n",
      "training mini-batch : 0 total loss = 1.1310793161392212\n",
      "epoch: 104 \n",
      " prediction training loss =  1.0030786771523326 validation loss =  1.0291758030653\n",
      "training mini-batch : 0 total loss = 0.9109169840812683\n",
      "epoch: 105 \n",
      " prediction training loss =  0.9969795691339594 validation loss =  1.030197262763977\n",
      "training mini-batch : 0 total loss = 0.9102790951728821\n",
      "epoch: 106 \n",
      " prediction training loss =  1.002681437291597 validation loss =  1.0297707617282867\n",
      "training mini-batch : 0 total loss = 1.0434556007385254\n",
      "epoch: 107 \n",
      " prediction training loss =  0.989070619407453 validation loss =  1.0293814539909363\n",
      "training mini-batch : 0 total loss = 1.0772149562835693\n",
      "epoch: 108 \n",
      " prediction training loss =  1.002197526003185 validation loss =  1.030124545097351\n",
      "training mini-batch : 0 total loss = 1.1596370935440063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 109 \n",
      " prediction training loss =  1.0017516675748324 validation loss =  1.031824752688408\n",
      "training mini-batch : 0 total loss = 1.14266037940979\n",
      "epoch: 110 \n",
      " prediction training loss =  1.008554386465173 validation loss =  1.0300065279006958\n",
      "training mini-batch : 0 total loss = 1.109254002571106\n",
      "epoch: 111 \n",
      " prediction training loss =  1.0034071897205554 validation loss =  1.0290019661188126\n",
      "training mini-batch : 0 total loss = 1.141083836555481\n",
      "epoch: 112 \n",
      " prediction training loss =  0.9960689387823406 validation loss =  1.0292624831199646\n",
      "training mini-batch : 0 total loss = 1.0529906749725342\n",
      "epoch: 113 \n",
      " prediction training loss =  1.0012633863248324 validation loss =  1.029240146279335\n",
      "training mini-batch : 0 total loss = 1.0887231826782227\n",
      "epoch: 114 \n",
      " prediction training loss =  1.0005893613162793 validation loss =  1.0286303460597992\n",
      "training mini-batch : 0 total loss = 1.0183131694793701\n",
      "epoch: 115 \n",
      " prediction training loss =  1.0009149971761202 validation loss =  1.0285432934761047\n",
      "training mini-batch : 0 total loss = 0.9969430565834045\n",
      "epoch: 116 \n",
      " prediction training loss =  0.9939462322937814 validation loss =  1.0295687466859818\n",
      "training mini-batch : 0 total loss = 1.089747667312622\n",
      "epoch: 117 \n",
      " prediction training loss =  1.005124619132594 validation loss =  1.030934289097786\n",
      "training mini-batch : 0 total loss = 1.1575590372085571\n",
      "epoch: 118 \n",
      " prediction training loss =  0.9961237907409668 validation loss =  1.0289351493120193\n",
      "training mini-batch : 0 total loss = 1.133691430091858\n",
      "epoch: 119 \n",
      " prediction training loss =  0.9946527041886982 validation loss =  1.0297376960515976\n",
      "training mini-batch : 0 total loss = 1.0017681121826172\n",
      "epoch: 120 \n",
      " prediction training loss =  1.0003734764299894 validation loss =  1.0294451713562012\n",
      "training mini-batch : 0 total loss = 1.04593026638031\n",
      "epoch: 121 \n",
      " prediction training loss =  1.0039154197040356 validation loss =  1.0316673815250397\n",
      "training mini-batch : 0 total loss = 1.0549534559249878\n",
      "epoch: 122 \n",
      " prediction training loss =  1.0001956098958065 validation loss =  1.0323732495307922\n",
      "training mini-batch : 0 total loss = 1.1142324209213257\n",
      "epoch: 123 \n",
      " prediction training loss =  0.9956587835362083 validation loss =  1.0298614501953125\n",
      "training mini-batch : 0 total loss = 0.9894675016403198\n",
      "epoch: 124 \n",
      " prediction training loss =  0.9970947096222326 validation loss =  1.0296993553638458\n",
      "training mini-batch : 0 total loss = 1.0530061721801758\n",
      "epoch: 125 \n",
      " prediction training loss =  1.0030755682995445 validation loss =  1.0290402919054031\n",
      "training mini-batch : 0 total loss = 0.9201176762580872\n",
      "epoch: 126 \n",
      " prediction training loss =  0.99951980615917 validation loss =  1.0304656624794006\n",
      "training mini-batch : 0 total loss = 0.9073328375816345\n",
      "epoch: 127 \n",
      " prediction training loss =  0.9972179061488101 validation loss =  1.0317620933055878\n",
      "training mini-batch : 0 total loss = 0.9610179662704468\n",
      "epoch: 128 \n",
      " prediction training loss =  1.0000298054594743 validation loss =  1.0313864946365356\n",
      "training mini-batch : 0 total loss = 1.027799367904663\n",
      "epoch: 129 \n",
      " prediction training loss =  1.0017362676168744 validation loss =  1.0315442979335785\n",
      "training mini-batch : 0 total loss = 1.0086963176727295\n",
      "epoch: 130 \n",
      " prediction training loss =  1.0004671498348838 validation loss =  1.0315410792827606\n",
      "training mini-batch : 0 total loss = 1.0246407985687256\n",
      "epoch: 131 \n",
      " prediction training loss =  0.9913462400436401 validation loss =  1.0312593579292297\n",
      "training mini-batch : 0 total loss = 1.0286859273910522\n",
      "epoch: 132 \n",
      " prediction training loss =  1.006543140662344 validation loss =  1.0318201333284378\n",
      "training mini-batch : 0 total loss = 1.116248607635498\n",
      "epoch: 133 \n",
      " prediction training loss =  0.9994098418637326 validation loss =  1.0310155153274536\n",
      "training mini-batch : 0 total loss = 0.9175211787223816\n",
      "epoch: 134 \n",
      " prediction training loss =  1.0007415288373043 validation loss =  1.0302019566297531\n",
      "training mini-batch : 0 total loss = 1.0535255670547485\n",
      "epoch: 135 \n",
      " prediction training loss =  0.9995154142379761 validation loss =  1.0310897082090378\n",
      "training mini-batch : 0 total loss = 0.9627081155776978\n",
      "epoch: 136 \n",
      " prediction training loss =  0.9955238825396487 validation loss =  1.0309516191482544\n",
      "training mini-batch : 0 total loss = 0.9717580676078796\n",
      "epoch: 137 \n",
      " prediction training loss =  0.9952390193939209 validation loss =  1.0326250493526459\n",
      "training mini-batch : 0 total loss = 1.0280758142471313\n",
      "epoch: 138 \n",
      " prediction training loss =  0.9976333850308469 validation loss =  1.0316374152898788\n",
      "training mini-batch : 0 total loss = 1.0540571212768555\n",
      "epoch: 139 \n",
      " prediction training loss =  1.0002101471549587 validation loss =  1.0309002250432968\n",
      "training mini-batch : 0 total loss = 0.8364320397377014\n",
      "epoch: 140 \n",
      " prediction training loss =  1.0031438344403316 validation loss =  1.0318924933671951\n",
      "training mini-batch : 0 total loss = 1.0615473985671997\n",
      "epoch: 141 \n",
      " prediction training loss =  0.9926427445913616 validation loss =  1.031755492091179\n",
      "training mini-batch : 0 total loss = 0.9978320598602295\n",
      "epoch: 142 \n",
      " prediction training loss =  1.002319922572688 validation loss =  1.030740037560463\n",
      "training mini-batch : 0 total loss = 1.1329364776611328\n",
      "epoch: 143 \n",
      " prediction training loss =  1.001303465742814 validation loss =  1.0301797837018967\n",
      "training mini-batch : 0 total loss = 1.0285389423370361\n",
      "epoch: 144 \n",
      " prediction training loss =  1.0007014400080632 validation loss =  1.0303268432617188\n",
      "training mini-batch : 0 total loss = 0.9415979385375977\n",
      "epoch: 145 \n",
      " prediction training loss =  0.9988649299270228 validation loss =  1.0315621942281723\n",
      "training mini-batch : 0 total loss = 1.199099063873291\n",
      "epoch: 146 \n",
      " prediction training loss =  1.0033948609703465 validation loss =  1.0320697128772736\n",
      "training mini-batch : 0 total loss = 1.0782662630081177\n",
      "epoch: 147 \n",
      " prediction training loss =  1.0007373000446118 validation loss =  1.0325338542461395\n",
      "training mini-batch : 0 total loss = 1.1021302938461304\n",
      "epoch: 148 \n",
      " prediction training loss =  0.9989405054795114 validation loss =  1.0318296998739243\n",
      "training mini-batch : 0 total loss = 0.9763199687004089\n",
      "epoch: 149 \n",
      " prediction training loss =  0.9951231636499104 validation loss =  1.0307555347681046\n",
      "training mini-batch : 0 total loss = 1.030367374420166\n",
      "epoch: 150 \n",
      " prediction training loss =  1.0137019910310443 validation loss =  1.032039687037468\n",
      "training mini-batch : 0 total loss = 0.9230108261108398\n",
      "epoch: 151 \n",
      " prediction training loss =  0.9945734707932723 validation loss =  1.0301122665405273\n",
      "training mini-batch : 0 total loss = 1.1484076976776123\n",
      "epoch: 152 \n",
      " prediction training loss =  1.0044974841569598 validation loss =  1.030714601278305\n",
      "training mini-batch : 0 total loss = 1.0832301378250122\n",
      "epoch: 153 \n",
      " prediction training loss =  1.0010301157047874 validation loss =  1.0310424715280533\n",
      "training mini-batch : 0 total loss = 1.0867328643798828\n",
      "epoch: 154 \n",
      " prediction training loss =  1.0024191047015942 validation loss =  1.0318745821714401\n",
      "training mini-batch : 0 total loss = 1.0000200271606445\n",
      "epoch: 155 \n",
      " prediction training loss =  0.9986028137959932 validation loss =  1.031007468700409\n",
      "training mini-batch : 0 total loss = 0.7952863574028015\n",
      "epoch: 156 \n",
      " prediction training loss =  0.9925296463464436 validation loss =  1.031278982758522\n",
      "training mini-batch : 0 total loss = 1.0224770307540894\n",
      "epoch: 157 \n",
      " prediction training loss =  0.9917551404551456 validation loss =  1.0309662371873856\n",
      "training mini-batch : 0 total loss = 0.8680599927902222\n",
      "epoch: 158 \n",
      " prediction training loss =  0.9994035457309923 validation loss =  1.0312059670686722\n",
      "training mini-batch : 0 total loss = 1.063781499862671\n",
      "epoch: 159 \n",
      " prediction training loss =  0.9970057136134097 validation loss =  1.0312444865703583\n",
      "training mini-batch : 0 total loss = 1.03415048122406\n",
      "epoch: 160 \n",
      " prediction training loss =  1.0056094182165045 validation loss =  1.0314797759056091\n",
      "training mini-batch : 0 total loss = 1.0144013166427612\n",
      "epoch: 161 \n",
      " prediction training loss =  0.9934968760139063 validation loss =  1.0324040353298187\n",
      "training mini-batch : 0 total loss = 0.899263858795166\n",
      "epoch: 162 \n",
      " prediction training loss =  0.9927141886008414 validation loss =  1.031783789396286\n",
      "training mini-batch : 0 total loss = 1.0433099269866943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 163 \n",
      " prediction training loss =  1.002071898234518 validation loss =  1.0314316898584366\n",
      "training mini-batch : 0 total loss = 1.0381031036376953\n",
      "epoch: 164 \n",
      " prediction training loss =  1.0041637702992088 validation loss =  1.0309756696224213\n",
      "training mini-batch : 0 total loss = 0.9103797674179077\n",
      "epoch: 165 \n",
      " prediction training loss =  0.9907927920943812 validation loss =  1.0307786017656326\n",
      "training mini-batch : 0 total loss = 1.130774974822998\n",
      "epoch: 166 \n",
      " prediction training loss =  0.9967580814110605 validation loss =  1.0310161858797073\n",
      "training mini-batch : 0 total loss = 0.9438854455947876\n",
      "epoch: 167 \n",
      " prediction training loss =  0.995817338165484 validation loss =  1.0312593877315521\n",
      "training mini-batch : 0 total loss = 0.9708418250083923\n",
      "epoch: 168 \n",
      " prediction training loss =  1.0024514731607939 validation loss =  1.031947836279869\n",
      "training mini-batch : 0 total loss = 0.8071081638336182\n",
      "epoch: 169 \n",
      " prediction training loss =  0.9946269894901075 validation loss =  1.0323478132486343\n",
      "training mini-batch : 0 total loss = 0.9321303963661194\n",
      "epoch: 170 \n",
      " prediction training loss =  0.9932453726467333 validation loss =  1.031768411397934\n",
      "training mini-batch : 0 total loss = 0.962600588798523\n",
      "epoch: 171 \n",
      " prediction training loss =  1.0068046105535406 validation loss =  1.0320819318294525\n",
      "training mini-batch : 0 total loss = 1.0945719480514526\n",
      "epoch: 172 \n",
      " prediction training loss =  1.005981959794697 validation loss =  1.0332439094781876\n",
      "training mini-batch : 0 total loss = 0.9092505574226379\n",
      "epoch: 173 \n",
      " prediction training loss =  1.0003723627642582 validation loss =  1.0316143482923508\n",
      "training mini-batch : 0 total loss = 1.0528534650802612\n",
      "epoch: 174 \n",
      " prediction training loss =  0.9949090700400504 validation loss =  1.0311558097600937\n",
      "training mini-batch : 0 total loss = 1.0551717281341553\n",
      "epoch: 175 \n",
      " prediction training loss =  1.011124102692855 validation loss =  1.0309742242097855\n",
      "training mini-batch : 0 total loss = 0.8488970398902893\n",
      "epoch: 176 \n",
      " prediction training loss =  0.9952792713516637 validation loss =  1.0312498658895493\n",
      "training mini-batch : 0 total loss = 0.9505131244659424\n",
      "epoch: 177 \n",
      " prediction training loss =  1.0010268750943636 validation loss =  1.0322296023368835\n",
      "training mini-batch : 0 total loss = 1.0511301755905151\n",
      "epoch: 178 \n",
      " prediction training loss =  0.9838133799402338 validation loss =  1.0316548347473145\n",
      "training mini-batch : 0 total loss = 1.1251850128173828\n",
      "epoch: 179 \n",
      " prediction training loss =  1.0024468647806268 validation loss =  1.032043695449829\n",
      "training mini-batch : 0 total loss = 0.7541466951370239\n",
      "epoch: 180 \n",
      " prediction training loss =  1.0057469763253863 validation loss =  1.0318527072668076\n",
      "training mini-batch : 0 total loss = 1.0708101987838745\n",
      "epoch: 181 \n",
      " prediction training loss =  0.9948303605380812 validation loss =  1.0319187492132187\n",
      "training mini-batch : 0 total loss = 0.9461861252784729\n",
      "epoch: 182 \n",
      " prediction training loss =  0.9871775194218284 validation loss =  1.0320700258016586\n",
      "training mini-batch : 0 total loss = 1.0729974508285522\n",
      "epoch: 183 \n",
      " prediction training loss =  1.0027481003811485 validation loss =  1.03090201318264\n",
      "training mini-batch : 0 total loss = 1.0684847831726074\n",
      "epoch: 184 \n",
      " prediction training loss =  0.9985491005997909 validation loss =  1.0292762368917465\n",
      "training mini-batch : 0 total loss = 0.9379738569259644\n",
      "epoch: 185 \n",
      " prediction training loss =  1.0042211319270886 validation loss =  1.0307065844535828\n",
      "training mini-batch : 0 total loss = 1.0008492469787598\n",
      "epoch: 186 \n",
      " prediction training loss =  0.9937984315972579 validation loss =  1.0310922712087631\n",
      "training mini-batch : 0 total loss = 0.9288977384567261\n",
      "epoch: 187 \n",
      " prediction training loss =  0.9934843559014169 validation loss =  1.0315912365913391\n",
      "training mini-batch : 0 total loss = 1.065261721611023\n",
      "epoch: 188 \n",
      " prediction training loss =  0.9957054602472406 validation loss =  1.0321236550807953\n",
      "training mini-batch : 0 total loss = 0.8489157557487488\n",
      "epoch: 189 \n",
      " prediction training loss =  1.002215696008582 validation loss =  1.0314818024635315\n",
      "training mini-batch : 0 total loss = 1.0089523792266846\n",
      "epoch: 190 \n",
      " prediction training loss =  0.995081016891881 validation loss =  1.0320773720741272\n",
      "training mini-batch : 0 total loss = 0.8856234550476074\n",
      "epoch: 191 \n",
      " prediction training loss =  1.0028177593883716 validation loss =  1.0331581979990005\n",
      "training mini-batch : 0 total loss = 0.9623964428901672\n",
      "epoch: 192 \n",
      " prediction training loss =  0.9931331276893616 validation loss =  1.033073052763939\n",
      "training mini-batch : 0 total loss = 1.168068766593933\n",
      "epoch: 193 \n",
      " prediction training loss =  0.9971824351109957 validation loss =  1.0323953032493591\n",
      "training mini-batch : 0 total loss = 1.0463356971740723\n",
      "epoch: 194 \n",
      " prediction training loss =  1.0032264213812978 validation loss =  1.0310950428247452\n",
      "training mini-batch : 0 total loss = 1.0914653539657593\n",
      "epoch: 195 \n",
      " prediction training loss =  1.0014887326642086 validation loss =  1.0311207920312881\n",
      "training mini-batch : 0 total loss = 1.018672227859497\n",
      "epoch: 196 \n",
      " prediction training loss =  1.000830546805733 validation loss =  1.0300410836935043\n",
      "training mini-batch : 0 total loss = 1.083886981010437\n",
      "epoch: 197 \n",
      " prediction training loss =  1.009791650270161 validation loss =  1.0305071473121643\n",
      "training mini-batch : 0 total loss = 0.9594943523406982\n",
      "epoch: 198 \n",
      " prediction training loss =  0.9956195762282923 validation loss =  1.03036767244339\n",
      "training mini-batch : 0 total loss = 1.0949223041534424\n",
      "epoch: 199 \n",
      " prediction training loss =  1.0048669074711047 validation loss =  1.032321259379387\n",
      "training mini-batch : 0 total loss = 0.9649314880371094\n",
      "epoch: 200 \n",
      " prediction training loss =  0.9976200900579754 validation loss =  1.0316942185163498\n",
      "training mini-batch : 0 total loss = 0.9703258275985718\n",
      "epoch: 201 \n",
      " prediction training loss =  1.0068456656054448 validation loss =  1.0296548753976822\n",
      "training mini-batch : 0 total loss = 1.0150915384292603\n",
      "epoch: 202 \n",
      " prediction training loss =  1.0004921461406506 validation loss =  1.0279172360897064\n",
      "training mini-batch : 0 total loss = 0.9620223045349121\n",
      "epoch: 203 \n",
      " prediction training loss =  1.0036985434983905 validation loss =  1.028946503996849\n",
      "training mini-batch : 0 total loss = 1.0017025470733643\n",
      "epoch: 204 \n",
      " prediction training loss =  0.9994094528649983 validation loss =  1.0298161953687668\n",
      "training mini-batch : 0 total loss = 0.8847790360450745\n",
      "epoch: 205 \n",
      " prediction training loss =  0.9931162972199289 validation loss =  1.0310282856225967\n",
      "training mini-batch : 0 total loss = 1.0360347032546997\n",
      "epoch: 206 \n",
      " prediction training loss =  1.0066229073624862 validation loss =  1.0308944284915924\n",
      "training mini-batch : 0 total loss = 0.8640753030776978\n",
      "epoch: 207 \n",
      " prediction training loss =  0.9985462521251879 validation loss =  1.0296617150306702\n",
      "training mini-batch : 0 total loss = 0.9128851294517517\n",
      "epoch: 208 \n",
      " prediction training loss =  0.9918167810691031 validation loss =  1.0292780250310898\n",
      "training mini-batch : 0 total loss = 0.872730016708374\n",
      "epoch: 209 \n",
      " prediction training loss =  0.9989624399887888 validation loss =  1.0295543670654297\n",
      "training mini-batch : 0 total loss = 1.0202451944351196\n",
      "epoch: 210 \n",
      " prediction training loss =  0.9975406402035764 validation loss =  1.0303185731172562\n",
      "training mini-batch : 0 total loss = 0.9769425988197327\n",
      "epoch: 211 \n",
      " prediction training loss =  1.0030054669631154 validation loss =  1.030692219734192\n",
      "training mini-batch : 0 total loss = 1.0722631216049194\n",
      "epoch: 212 \n",
      " prediction training loss =  0.997444676725488 validation loss =  1.031291663646698\n",
      "training mini-batch : 0 total loss = 0.864570140838623\n",
      "epoch: 213 \n",
      " prediction training loss =  0.9900854825973511 validation loss =  1.031447097659111\n",
      "training mini-batch : 0 total loss = 0.911664605140686\n",
      "epoch: 214 \n",
      " prediction training loss =  0.9978884019349751 validation loss =  1.0329672992229462\n",
      "training mini-batch : 0 total loss = 1.0194957256317139\n",
      "epoch: 215 \n",
      " prediction training loss =  0.9982748376695734 validation loss =  1.0331102460622787\n",
      "training mini-batch : 0 total loss = 0.9881360530853271\n",
      "epoch: 216 \n",
      " prediction training loss =  0.9947436953845777 validation loss =  1.0312811732292175\n",
      "training mini-batch : 0 total loss = 1.0712238550186157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 217 \n",
      " prediction training loss =  0.9972720899079975 validation loss =  1.0330114215612411\n",
      "training mini-batch : 0 total loss = 0.9824677109718323\n",
      "epoch: 218 \n",
      " prediction training loss =  1.000550549281271 validation loss =  1.0315647274255753\n",
      "training mini-batch : 0 total loss = 0.9970460534095764\n",
      "epoch: 219 \n",
      " prediction training loss =  1.0001706292754726 validation loss =  1.03220996260643\n",
      "training mini-batch : 0 total loss = 0.9417990446090698\n",
      "epoch: 220 \n",
      " prediction training loss =  0.9995990960221541 validation loss =  1.0321243405342102\n",
      "training mini-batch : 0 total loss = 0.6460378170013428\n",
      "epoch: 221 \n",
      " prediction training loss =  0.9987319770612215 validation loss =  1.0317118167877197\n",
      "training mini-batch : 0 total loss = 1.002706527709961\n",
      "epoch: 222 \n",
      " prediction training loss =  1.0079656180582548 validation loss =  1.030911073088646\n",
      "training mini-batch : 0 total loss = 1.0585566759109497\n",
      "epoch: 223 \n",
      " prediction training loss =  0.9899667501449585 validation loss =  1.0305300056934357\n",
      "training mini-batch : 0 total loss = 1.0502058267593384\n",
      "epoch: 224 \n",
      " prediction training loss =  0.999044562640943 validation loss =  1.0320919007062912\n",
      "training mini-batch : 0 total loss = 1.180543065071106\n",
      "epoch: 225 \n",
      " prediction training loss =  0.9993987114805924 validation loss =  1.0320101827383041\n",
      "training mini-batch : 0 total loss = 1.0190197229385376\n",
      "epoch: 226 \n",
      " prediction training loss =  0.9943594963927018 validation loss =  1.0310650318861008\n",
      "training mini-batch : 0 total loss = 0.9627571105957031\n",
      "epoch: 227 \n",
      " prediction training loss =  0.9938514546344155 validation loss =  1.0326601415872574\n",
      "training mini-batch : 0 total loss = 1.0000896453857422\n",
      "epoch: 228 \n",
      " prediction training loss =  0.9963787260808443 validation loss =  1.030015841126442\n",
      "training mini-batch : 0 total loss = 1.0371547937393188\n",
      "epoch: 229 \n",
      " prediction training loss =  1.0039865092227334 validation loss =  1.0304955840110779\n",
      "training mini-batch : 0 total loss = 0.9566736817359924\n",
      "epoch: 230 \n",
      " prediction training loss =  0.9965579164655585 validation loss =  1.0315826833248138\n",
      "training mini-batch : 0 total loss = 0.8567888140678406\n",
      "epoch: 231 \n",
      " prediction training loss =  0.984929831404435 validation loss =  1.0299817323684692\n",
      "training mini-batch : 0 total loss = 0.9192643165588379\n",
      "epoch: 232 \n",
      " prediction training loss =  1.004155196641621 validation loss =  1.0301935225725174\n",
      "training mini-batch : 0 total loss = 0.9544686079025269\n",
      "epoch: 233 \n",
      " prediction training loss =  1.0014457451669794 validation loss =  1.0329093486070633\n",
      "training mini-batch : 0 total loss = 0.9495254158973694\n",
      "epoch: 234 \n",
      " prediction training loss =  0.9984368619165922 validation loss =  1.0311831533908844\n",
      "training mini-batch : 0 total loss = 0.912739098072052\n",
      "epoch: 235 \n",
      " prediction training loss =  1.0017984321242885 validation loss =  1.0304805636405945\n",
      "training mini-batch : 0 total loss = 0.8672354817390442\n",
      "epoch: 236 \n",
      " prediction training loss =  1.001939412794615 validation loss =  1.0313879996538162\n",
      "training mini-batch : 0 total loss = 0.9252868890762329\n",
      "epoch: 237 \n",
      " prediction training loss =  1.001190838060881 validation loss =  1.031007245182991\n",
      "training mini-batch : 0 total loss = 0.944798469543457\n",
      "epoch: 238 \n",
      " prediction training loss =  0.9939095566147252 validation loss =  1.0304225236177444\n",
      "training mini-batch : 0 total loss = 1.1002018451690674\n",
      "epoch: 239 \n",
      " prediction training loss =  0.994983434677124 validation loss =  1.0334484577178955\n",
      "training mini-batch : 0 total loss = 0.9470182061195374\n",
      "epoch: 240 \n",
      " prediction training loss =  0.9987201000514784 validation loss =  1.0308991819620132\n",
      "training mini-batch : 0 total loss = 1.0124216079711914\n",
      "epoch: 241 \n",
      " prediction training loss =  0.9992295723212393 validation loss =  1.0306795090436935\n",
      "training mini-batch : 0 total loss = 0.8758956789970398\n",
      "epoch: 242 \n",
      " prediction training loss =  0.9916414022445679 validation loss =  1.0303752720355988\n",
      "training mini-batch : 0 total loss = 1.0293251276016235\n",
      "epoch: 243 \n",
      " prediction training loss =  0.9933915702920211 validation loss =  1.0270781964063644\n",
      "training mini-batch : 0 total loss = 0.9043691754341125\n",
      "epoch: 244 \n",
      " prediction training loss =  0.9978619594323007 validation loss =  1.0282316654920578\n",
      "training mini-batch : 0 total loss = 0.96346515417099\n",
      "epoch: 245 \n",
      " prediction training loss =  0.9923671672218725 validation loss =  1.0319829136133194\n",
      "training mini-batch : 0 total loss = 0.9396409392356873\n",
      "epoch: 246 \n",
      " prediction training loss =  1.0068630168312473 validation loss =  1.0345452725887299\n",
      "training mini-batch : 0 total loss = 0.9554243087768555\n",
      "epoch: 247 \n",
      " prediction training loss =  0.9885830440019306 validation loss =  1.0331675708293915\n",
      "training mini-batch : 0 total loss = 0.8070610165596008\n",
      "epoch: 248 \n",
      " prediction training loss =  0.9958434732336747 validation loss =  1.0317291766405106\n",
      "training mini-batch : 0 total loss = 1.0176795721054077\n",
      "epoch: 249 \n",
      " prediction training loss =  0.9978034715903433 validation loss =  1.0325818359851837\n",
      "training mini-batch : 0 total loss = 0.9192513227462769\n",
      "epoch: 250 \n",
      " prediction training loss =  1.0011699136934782 validation loss =  1.032943531870842\n",
      "training mini-batch : 0 total loss = 1.0079460144042969\n",
      "epoch: 251 \n",
      " prediction training loss =  0.9997704468275371 validation loss =  1.0320318788290024\n",
      "training mini-batch : 0 total loss = 1.2147845029830933\n",
      "epoch: 252 \n",
      " prediction training loss =  1.006553408346678 validation loss =  1.0311293303966522\n",
      "training mini-batch : 0 total loss = 0.9502150416374207\n",
      "epoch: 253 \n",
      " prediction training loss =  0.9956620335578918 validation loss =  1.0339394956827164\n",
      "training mini-batch : 0 total loss = 1.015714168548584\n",
      "epoch: 254 \n",
      " prediction training loss =  1.000345057562778 validation loss =  1.0308638215065002\n",
      "training mini-batch : 0 total loss = 0.9122788906097412\n",
      "epoch: 255 \n",
      " prediction training loss =  1.0039830145082975 validation loss =  1.0306661874055862\n",
      "training mini-batch : 0 total loss = 0.8880323171615601\n",
      "epoch: 256 \n",
      " prediction training loss =  1.0047738207013983 validation loss =  1.0321489721536636\n",
      "training mini-batch : 0 total loss = 0.9979734420776367\n",
      "epoch: 257 \n",
      " prediction training loss =  1.0010303164783276 validation loss =  1.0312629491090775\n",
      "training mini-batch : 0 total loss = 1.0125782489776611\n",
      "epoch: 258 \n",
      " prediction training loss =  1.000806494763023 validation loss =  1.0317255407571793\n",
      "training mini-batch : 0 total loss = 0.9679879546165466\n",
      "epoch: 259 \n",
      " prediction training loss =  1.0002392153990896 validation loss =  1.031502678990364\n",
      "training mini-batch : 0 total loss = 0.9015004634857178\n",
      "epoch: 260 \n",
      " prediction training loss =  0.9953356918535734 validation loss =  1.0307853668928146\n",
      "training mini-batch : 0 total loss = 0.9408288598060608\n",
      "epoch: 261 \n",
      " prediction training loss =  0.9954435480268378 validation loss =  1.0312906503677368\n",
      "training mini-batch : 0 total loss = 1.0558958053588867\n",
      "epoch: 262 \n",
      " prediction training loss =  0.9980926356817547 validation loss =  1.0328282564878464\n",
      "training mini-batch : 0 total loss = 0.8978893756866455\n",
      "epoch: 263 \n",
      " prediction training loss =  1.0006580478266667 validation loss =  1.0322559177875519\n",
      "training mini-batch : 0 total loss = 1.1837574243545532\n",
      "epoch: 264 \n",
      " prediction training loss =  1.0017646049198352 validation loss =  1.0320445150136948\n",
      "training mini-batch : 0 total loss = 0.9110859632492065\n",
      "epoch: 265 \n",
      " prediction training loss =  0.9945533808908964 validation loss =  1.0312800109386444\n",
      "training mini-batch : 0 total loss = 1.0399450063705444\n",
      "epoch: 266 \n",
      " prediction training loss =  0.9965818743956717 validation loss =  1.0322514176368713\n",
      "training mini-batch : 0 total loss = 0.921141505241394\n",
      "epoch: 267 \n",
      " prediction training loss =  0.9974222559677927 validation loss =  1.0323382467031479\n",
      "training mini-batch : 0 total loss = 0.9956123232841492\n",
      "epoch: 268 \n",
      " prediction training loss =  1.0085956015084918 validation loss =  1.032075583934784\n",
      "training mini-batch : 0 total loss = 0.9221324324607849\n",
      "epoch: 269 \n",
      " prediction training loss =  1.0053494070705615 validation loss =  1.032092347741127\n",
      "training mini-batch : 0 total loss = 0.8065344095230103\n",
      "epoch: 270 \n",
      " prediction training loss =  0.9978722459391544 validation loss =  1.0313956141471863\n",
      "training mini-batch : 0 total loss = 1.0187256336212158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 271 \n",
      " prediction training loss =  0.9935453408642819 validation loss =  1.0313407480716705\n",
      "training mini-batch : 0 total loss = 0.9216467142105103\n",
      "epoch: 272 \n",
      " prediction training loss =  0.9959329962730408 validation loss =  1.0304900854825974\n",
      "training mini-batch : 0 total loss = 1.0318812131881714\n",
      "epoch: 273 \n",
      " prediction training loss =  0.9988675117492676 validation loss =  1.0316057354211807\n",
      "training mini-batch : 0 total loss = 1.0350441932678223\n",
      "epoch: 274 \n",
      " prediction training loss =  0.9936130893857855 validation loss =  1.0337518453598022\n",
      "training mini-batch : 0 total loss = 1.175463318824768\n",
      "epoch: 275 \n",
      " prediction training loss =  1.0014336171903109 validation loss =  1.032255545258522\n",
      "training mini-batch : 0 total loss = 1.080377459526062\n",
      "epoch: 276 \n",
      " prediction training loss =  0.9938853508547733 validation loss =  1.0312416851520538\n",
      "training mini-batch : 0 total loss = 1.0780726671218872\n",
      "epoch: 277 \n",
      " prediction training loss =  0.9977647066116333 validation loss =  1.0324119180440903\n",
      "training mini-batch : 0 total loss = 1.123716115951538\n",
      "epoch: 278 \n",
      " prediction training loss =  0.9982868527111254 validation loss =  1.0341566652059555\n",
      "training mini-batch : 0 total loss = 1.0924938917160034\n",
      "epoch: 279 \n",
      " prediction training loss =  0.9965582584079943 validation loss =  1.0329084992408752\n",
      "training mini-batch : 0 total loss = 0.995123565196991\n",
      "epoch: 280 \n",
      " prediction training loss =  1.0036481838477285 validation loss =  1.0323302149772644\n",
      "training mini-batch : 0 total loss = 1.0945698022842407\n",
      "epoch: 281 \n",
      " prediction training loss =  0.9980503006985313 validation loss =  1.0322821587324142\n",
      "training mini-batch : 0 total loss = 1.0030921697616577\n",
      "epoch: 282 \n",
      " prediction training loss =  0.9969046994259483 validation loss =  1.0326945036649704\n",
      "training mini-batch : 0 total loss = 0.9221779108047485\n",
      "epoch: 283 \n",
      " prediction training loss =  0.9901424144443712 validation loss =  1.0313652753829956\n",
      "training mini-batch : 0 total loss = 1.0949647426605225\n",
      "epoch: 284 \n",
      " prediction training loss =  1.0003959693406757 validation loss =  1.031579077243805\n",
      "training mini-batch : 0 total loss = 1.0023548603057861\n",
      "epoch: 285 \n",
      " prediction training loss =  0.9937954011716341 validation loss =  1.032780796289444\n",
      "training mini-batch : 0 total loss = 0.976381242275238\n",
      "epoch: 286 \n",
      " prediction training loss =  1.0010702578645003 validation loss =  1.034212127327919\n",
      "training mini-batch : 0 total loss = 0.9713473916053772\n",
      "epoch: 287 \n",
      " prediction training loss =  0.9952714756915444 validation loss =  1.0316070020198822\n",
      "training mini-batch : 0 total loss = 1.07439386844635\n",
      "epoch: 288 \n",
      " prediction training loss =  0.9935624630827653 validation loss =  1.032321274280548\n",
      "training mini-batch : 0 total loss = 0.8390678763389587\n",
      "epoch: 289 \n",
      " prediction training loss =  0.9966210534698084 validation loss =  1.0326490253210068\n",
      "training mini-batch : 0 total loss = 0.9806467294692993\n",
      "epoch: 290 \n",
      " prediction training loss =  0.9974645627172369 validation loss =  1.032302439212799\n",
      "training mini-batch : 0 total loss = 0.9080569744110107\n",
      "epoch: 291 \n",
      " prediction training loss =  1.0091432615330345 validation loss =  1.0323259681463242\n",
      "training mini-batch : 0 total loss = 0.9229840636253357\n",
      "epoch: 292 \n",
      " prediction training loss =  0.9916080550143593 validation loss =  1.0321291983127594\n",
      "training mini-batch : 0 total loss = 0.8624401092529297\n",
      "epoch: 293 \n",
      " prediction training loss =  1.006168349793083 validation loss =  1.036891520023346\n",
      "training mini-batch : 0 total loss = 0.9932134747505188\n",
      "epoch: 294 \n",
      " prediction training loss =  0.9946228203020597 validation loss =  1.0337925851345062\n",
      "training mini-batch : 0 total loss = 0.9879245758056641\n",
      "epoch: 295 \n",
      " prediction training loss =  0.9918954278293409 validation loss =  1.031267374753952\n",
      "training mini-batch : 0 total loss = 0.862542986869812\n",
      "epoch: 296 \n",
      " prediction training loss =  1.0008962593580548 validation loss =  1.029547095298767\n",
      "training mini-batch : 0 total loss = 0.9050716757774353\n",
      "epoch: 297 \n",
      " prediction training loss =  1.0000522324913426 validation loss =  1.033734381198883\n",
      "training mini-batch : 0 total loss = 0.9560607075691223\n",
      "epoch: 298 \n",
      " prediction training loss =  0.9964450566392196 validation loss =  1.031000167131424\n",
      "training mini-batch : 0 total loss = 1.0386416912078857\n",
      "epoch: 299 \n",
      " prediction training loss =  0.9958388868131136 validation loss =  1.0314341485500336\n",
      "training mini-batch : 0 total loss = 0.9645980596542358\n",
      "epoch: 300 \n",
      " prediction training loss =  1.0009253087796663 validation loss =  1.0316787958145142\n",
      "training mini-batch : 0 total loss = 0.9617282748222351\n",
      "epoch: 301 \n",
      " prediction training loss =  1.000062722908823 validation loss =  1.0309001952409744\n",
      "training mini-batch : 0 total loss = 0.9111701250076294\n",
      "epoch: 302 \n",
      " prediction training loss =  0.995057789902938 validation loss =  1.0319205671548843\n",
      "training mini-batch : 0 total loss = 0.8860308527946472\n",
      "epoch: 303 \n",
      " prediction training loss =  1.0043843074848777 validation loss =  1.0324051082134247\n",
      "training mini-batch : 0 total loss = 1.020043134689331\n",
      "epoch: 304 \n",
      " prediction training loss =  0.9992571755459434 validation loss =  1.0338429510593414\n",
      "training mini-batch : 0 total loss = 1.0926867723464966\n",
      "epoch: 305 \n",
      " prediction training loss =  0.9994780860449138 validation loss =  1.0319142937660217\n",
      "training mini-batch : 0 total loss = 0.9527634978294373\n",
      "epoch: 306 \n",
      " prediction training loss =  0.997154929135975 validation loss =  1.0305492579936981\n",
      "training mini-batch : 0 total loss = 1.136549711227417\n",
      "epoch: 307 \n",
      " prediction training loss =  0.9943945439238298 validation loss =  1.030275285243988\n",
      "training mini-batch : 0 total loss = 1.199548363685608\n",
      "epoch: 308 \n",
      " prediction training loss =  0.9997711903170535 validation loss =  1.0304229855537415\n",
      "training mini-batch : 0 total loss = 0.935689389705658\n",
      "epoch: 309 \n",
      " prediction training loss =  1.0034635725774264 validation loss =  1.0315732806921005\n",
      "training mini-batch : 0 total loss = 0.9834945797920227\n",
      "epoch: 310 \n",
      " prediction training loss =  0.992595186359004 validation loss =  1.0308604687452316\n",
      "training mini-batch : 0 total loss = 0.8670421838760376\n",
      "epoch: 311 \n",
      " prediction training loss =  0.9955447849474455 validation loss =  1.0308081060647964\n",
      "training mini-batch : 0 total loss = 1.141796588897705\n",
      "epoch: 312 \n",
      " prediction training loss =  0.9908758652837653 validation loss =  1.0310118943452835\n",
      "training mini-batch : 0 total loss = 1.0793228149414062\n",
      "epoch: 313 \n",
      " prediction training loss =  0.9982460705857528 validation loss =  1.0312552452087402\n",
      "training mini-batch : 0 total loss = 0.9581317901611328\n",
      "epoch: 314 \n",
      " prediction training loss =  0.9953188864808333 validation loss =  1.0324295610189438\n",
      "training mini-batch : 0 total loss = 1.04007887840271\n",
      "epoch: 315 \n",
      " prediction training loss =  1.000030241514507 validation loss =  1.0331696420907974\n",
      "training mini-batch : 0 total loss = 0.9705413579940796\n",
      "epoch: 316 \n",
      " prediction training loss =  0.9959887924947237 validation loss =  1.0313970446586609\n",
      "training mini-batch : 0 total loss = 1.0478321313858032\n",
      "epoch: 317 \n",
      " prediction training loss =  1.0088644717869006 validation loss =  1.0313147008419037\n",
      "training mini-batch : 0 total loss = 0.9758650064468384\n",
      "epoch: 318 \n",
      " prediction training loss =  1.0018104973592257 validation loss =  1.030233308672905\n",
      "training mini-batch : 0 total loss = 0.895906925201416\n",
      "epoch: 319 \n",
      " prediction training loss =  0.9919094694288153 validation loss =  1.032266691327095\n",
      "training mini-batch : 0 total loss = 1.0614778995513916\n",
      "epoch: 320 \n",
      " prediction training loss =  0.999580464865032 validation loss =  1.0322901457548141\n",
      "training mini-batch : 0 total loss = 0.8941270709037781\n",
      "epoch: 321 \n",
      " prediction training loss =  0.9895695573405215 validation loss =  1.032001718878746\n",
      "training mini-batch : 0 total loss = 0.8787741661071777\n",
      "epoch: 322 \n",
      " prediction training loss =  0.9977162229387384 validation loss =  1.0308873653411865\n",
      "training mini-batch : 0 total loss = 1.0392847061157227\n",
      "epoch: 323 \n",
      " prediction training loss =  0.9983342540891547 validation loss =  1.031409740447998\n",
      "training mini-batch : 0 total loss = 0.8800318241119385\n",
      "epoch: 324 \n",
      " prediction training loss =  0.9960904497849313 validation loss =  1.03380486369133\n",
      "training mini-batch : 0 total loss = 1.1193501949310303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 325 \n",
      " prediction training loss =  0.9964531283629569 validation loss =  1.0323982387781143\n",
      "training mini-batch : 0 total loss = 0.9324261546134949\n",
      "epoch: 326 \n",
      " prediction training loss =  0.9967679349999679 validation loss =  1.032160222530365\n",
      "training mini-batch : 0 total loss = 1.2363970279693604\n",
      "epoch: 327 \n",
      " prediction training loss =  0.9959134114416022 validation loss =  1.0328474044799805\n",
      "training mini-batch : 0 total loss = 1.048978567123413\n",
      "epoch: 328 \n",
      " prediction training loss =  1.0046027145887677 validation loss =  1.0331144481897354\n",
      "training mini-batch : 0 total loss = 1.0459706783294678\n",
      "epoch: 329 \n",
      " prediction training loss =  1.00743252666373 validation loss =  1.0294098407030106\n",
      "training mini-batch : 0 total loss = 1.0672038793563843\n",
      "epoch: 330 \n",
      " prediction training loss =  0.9894904462914718 validation loss =  1.0315016508102417\n",
      "training mini-batch : 0 total loss = 0.8413147926330566\n",
      "epoch: 331 \n",
      " prediction training loss =  0.9931234497773019 validation loss =  1.0326281934976578\n",
      "training mini-batch : 0 total loss = 0.98333740234375\n",
      "epoch: 332 \n",
      " prediction training loss =  0.9895189373116744 validation loss =  1.032485082745552\n",
      "training mini-batch : 0 total loss = 0.9518542885780334\n",
      "epoch: 333 \n",
      " prediction training loss =  1.0058558426405255 validation loss =  1.033895418047905\n",
      "training mini-batch : 0 total loss = 1.0915452241897583\n",
      "epoch: 334 \n",
      " prediction training loss =  1.0029381984158565 validation loss =  1.0340601354837418\n",
      "training mini-batch : 0 total loss = 1.0231273174285889\n",
      "epoch: 335 \n",
      " prediction training loss =  0.9975865796992653 validation loss =  1.031553477048874\n",
      "training mini-batch : 0 total loss = 1.2128047943115234\n",
      "epoch: 336 \n",
      " prediction training loss =  1.0053553832204718 validation loss =  1.0327474623918533\n",
      "training mini-batch : 0 total loss = 0.8757495880126953\n",
      "epoch: 337 \n",
      " prediction training loss =  1.010391276133688 validation loss =  1.032540500164032\n",
      "training mini-batch : 0 total loss = 0.9362726807594299\n",
      "epoch: 338 \n",
      " prediction training loss =  0.993994581071954 validation loss =  1.0327374786138535\n",
      "training mini-batch : 0 total loss = 1.107507586479187\n",
      "epoch: 339 \n",
      " prediction training loss =  0.9970842568497909 validation loss =  1.0328402519226074\n",
      "training mini-batch : 0 total loss = 0.9056143760681152\n",
      "epoch: 340 \n",
      " prediction training loss =  0.9938004895260459 validation loss =  1.0314034521579742\n",
      "training mini-batch : 0 total loss = 1.0420914888381958\n",
      "epoch: 341 \n",
      " prediction training loss =  1.003379275924281 validation loss =  1.0329692214727402\n",
      "training mini-batch : 0 total loss = 0.7846778631210327\n",
      "epoch: 342 \n",
      " prediction training loss =  1.0103412145062496 validation loss =  1.0323814004659653\n",
      "training mini-batch : 0 total loss = 1.0317732095718384\n",
      "epoch: 343 \n",
      " prediction training loss =  0.9964897256148489 validation loss =  1.0364234298467636\n",
      "training mini-batch : 0 total loss = 0.956879734992981\n",
      "epoch: 344 \n",
      " prediction training loss =  0.9892727450320595 validation loss =  1.0343450456857681\n",
      "training mini-batch : 0 total loss = 1.0665146112442017\n",
      "epoch: 345 \n",
      " prediction training loss =  1.0007698912369578 validation loss =  1.0342288166284561\n",
      "training mini-batch : 0 total loss = 0.9484747648239136\n",
      "epoch: 346 \n",
      " prediction training loss =  0.9927272420180472 validation loss =  1.0346792042255402\n",
      "training mini-batch : 0 total loss = 0.9518548250198364\n",
      "epoch: 347 \n",
      " prediction training loss =  0.9962160806906851 validation loss =  1.032590001821518\n",
      "training mini-batch : 0 total loss = 0.8978307843208313\n",
      "epoch: 348 \n",
      " prediction training loss =  0.9966854421715987 validation loss =  1.0332886427640915\n",
      "training mini-batch : 0 total loss = 1.1174603700637817\n",
      "epoch: 349 \n",
      " prediction training loss =  0.9993632122089988 validation loss =  1.0325895249843597\n",
      "training mini-batch : 0 total loss = 0.9608891010284424\n",
      "epoch: 350 \n",
      " prediction training loss =  0.99648884723061 validation loss =  1.0326254218816757\n",
      "training mini-batch : 0 total loss = 0.8936000466346741\n",
      "epoch: 351 \n",
      " prediction training loss =  0.991680527988233 validation loss =  1.0311624705791473\n",
      "training mini-batch : 0 total loss = 0.8831074833869934\n",
      "epoch: 352 \n",
      " prediction training loss =  1.0001451028020758 validation loss =  1.0317513048648834\n",
      "training mini-batch : 0 total loss = 0.9534801244735718\n",
      "epoch: 353 \n",
      " prediction training loss =  0.990761094971707 validation loss =  1.033006191253662\n",
      "training mini-batch : 0 total loss = 1.1361727714538574\n",
      "epoch: 354 \n",
      " prediction training loss =  0.9954441503474587 validation loss =  1.0327339619398117\n",
      "training mini-batch : 0 total loss = 1.0255706310272217\n",
      "epoch: 355 \n",
      " prediction training loss =  0.9902758253248114 validation loss =  1.031845211982727\n",
      "training mini-batch : 0 total loss = 0.9826865792274475\n",
      "epoch: 356 \n",
      " prediction training loss =  0.9925851006256906 validation loss =  1.0321124643087387\n",
      "training mini-batch : 0 total loss = 1.019870638847351\n",
      "epoch: 357 \n",
      " prediction training loss =  1.003419157705809 validation loss =  1.035270020365715\n",
      "training mini-batch : 0 total loss = 0.8926200866699219\n",
      "epoch: 358 \n",
      " prediction training loss =  1.0033194109013206 validation loss =  1.0322369784116745\n",
      "training mini-batch : 0 total loss = 0.9019885063171387\n",
      "epoch: 359 \n",
      " prediction training loss =  0.9929869394553336 validation loss =  1.033316656947136\n",
      "training mini-batch : 0 total loss = 1.051103949546814\n",
      "epoch: 360 \n",
      " prediction training loss =  0.9967478356863323 validation loss =  1.0325266122817993\n",
      "training mini-batch : 0 total loss = 1.0844428539276123\n",
      "epoch: 361 \n",
      " prediction training loss =  0.9843442910595944 validation loss =  1.0313317626714706\n",
      "training mini-batch : 0 total loss = 0.9625764489173889\n",
      "epoch: 362 \n",
      " prediction training loss =  1.0011988752766658 validation loss =  1.033824309706688\n",
      "training mini-batch : 0 total loss = 1.027021884918213\n",
      "epoch: 363 \n",
      " prediction training loss =  0.9967583229667262 validation loss =  1.035477176308632\n",
      "training mini-batch : 0 total loss = 1.1128933429718018\n",
      "epoch: 364 \n",
      " prediction training loss =  0.9963725021010951 validation loss =  1.0337011814117432\n",
      "training mini-batch : 0 total loss = 1.089996576309204\n",
      "epoch: 365 \n",
      " prediction training loss =  0.9995340202984057 validation loss =  1.0328694880008698\n",
      "training mini-batch : 0 total loss = 0.9797558784484863\n",
      "epoch: 366 \n",
      " prediction training loss =  0.997121977178674 validation loss =  1.0331868082284927\n",
      "training mini-batch : 0 total loss = 0.9314591884613037\n",
      "epoch: 367 \n",
      " prediction training loss =  1.0056993710367303 validation loss =  1.037274494767189\n",
      "training mini-batch : 0 total loss = 0.9714474081993103\n",
      "epoch: 368 \n",
      " prediction training loss =  0.9982244121400934 validation loss =  1.028622418642044\n",
      "training mini-batch : 0 total loss = 0.9340035319328308\n",
      "epoch: 369 \n",
      " prediction training loss =  0.9930163716015062 validation loss =  1.0367283076047897\n",
      "training mini-batch : 0 total loss = 0.9369350671768188\n",
      "epoch: 370 \n",
      " prediction training loss =  0.9865464411283794 validation loss =  1.034230262041092\n",
      "training mini-batch : 0 total loss = 1.0418168306350708\n",
      "epoch: 371 \n",
      " prediction training loss =  1.0037341839388798 validation loss =  1.0370917171239853\n",
      "training mini-batch : 0 total loss = 0.9614725708961487\n",
      "epoch: 372 \n",
      " prediction training loss =  0.9997382101259733 validation loss =  1.0322777181863785\n",
      "training mini-batch : 0 total loss = 0.8872283697128296\n",
      "epoch: 373 \n",
      " prediction training loss =  0.9954477768195303 validation loss =  1.03748320043087\n",
      "training mini-batch : 0 total loss = 1.1113749742507935\n",
      "epoch: 374 \n",
      " prediction training loss =  0.9961461769907098 validation loss =  1.0356604307889938\n",
      "training mini-batch : 0 total loss = 1.013778805732727\n",
      "epoch: 375 \n",
      " prediction training loss =  0.999666944930428 validation loss =  1.0342140793800354\n",
      "training mini-batch : 0 total loss = 1.0018885135650635\n",
      "epoch: 376 \n",
      " prediction training loss =  1.0000913896058734 validation loss =  1.0351857244968414\n",
      "training mini-batch : 0 total loss = 1.016740083694458\n",
      "epoch: 377 \n",
      " prediction training loss =  0.9998713292573628 validation loss =  1.0345848351716995\n",
      "training mini-batch : 0 total loss = 1.0179146528244019\n",
      "epoch: 378 \n",
      " prediction training loss =  1.00042630496778 validation loss =  1.0345898270606995\n",
      "training mini-batch : 0 total loss = 0.909643292427063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 379 \n",
      " prediction training loss =  1.0040924800069708 validation loss =  1.0375242233276367\n",
      "training mini-batch : 0 total loss = 1.1955338716506958\n",
      "epoch: 380 \n",
      " prediction training loss =  1.0046806272707487 validation loss =  1.034661814570427\n",
      "training mini-batch : 0 total loss = 0.8803425431251526\n",
      "epoch: 381 \n",
      " prediction training loss =  0.999743872567227 validation loss =  1.0364440828561783\n",
      "training mini-batch : 0 total loss = 0.8970231413841248\n",
      "epoch: 382 \n",
      " prediction training loss =  0.9901566756399054 validation loss =  1.0348210334777832\n",
      "training mini-batch : 0 total loss = 1.1056022644042969\n",
      "epoch: 383 \n",
      " prediction training loss =  0.998077320425134 validation loss =  1.029746025800705\n",
      "training mini-batch : 0 total loss = 0.8832205533981323\n",
      "epoch: 384 \n",
      " prediction training loss =  0.9942837922196639 validation loss =  1.0342498123645782\n",
      "training mini-batch : 0 total loss = 1.1352193355560303\n",
      "epoch: 385 \n",
      " prediction training loss =  1.0010041437650983 validation loss =  1.0324253141880035\n",
      "training mini-batch : 0 total loss = 0.9208264350891113\n",
      "epoch: 386 \n",
      " prediction training loss =  0.9950176822511774 validation loss =  1.0325911343097687\n",
      "training mini-batch : 0 total loss = 1.0278701782226562\n",
      "epoch: 387 \n",
      " prediction training loss =  0.9995741404985127 validation loss =  1.034581258893013\n",
      "training mini-batch : 0 total loss = 1.0694551467895508\n",
      "epoch: 388 \n",
      " prediction training loss =  0.9948369546940452 validation loss =  1.0332106947898865\n",
      "training mini-batch : 0 total loss = 1.014406442642212\n",
      "epoch: 389 \n",
      " prediction training loss =  0.9929527577600981 validation loss =  1.0338839888572693\n",
      "training mini-batch : 0 total loss = 1.0134207010269165\n",
      "epoch: 390 \n",
      " prediction training loss =  0.9859405191321122 validation loss =  1.0330722481012344\n",
      "training mini-batch : 0 total loss = 1.0896480083465576\n",
      "epoch: 391 \n",
      " prediction training loss =  0.9900044359658894 validation loss =  1.0341258645057678\n",
      "training mini-batch : 0 total loss = 0.9353255033493042\n",
      "epoch: 392 \n",
      " prediction training loss =  0.9865513757655495 validation loss =  1.0351541340351105\n",
      "training mini-batch : 0 total loss = 0.9191604852676392\n",
      "epoch: 393 \n",
      " prediction training loss =  0.9951952507621363 validation loss =  1.0360213965177536\n",
      "training mini-batch : 0 total loss = 0.8918153047561646\n",
      "epoch: 394 \n",
      " prediction training loss =  0.9904530424820749 validation loss =  1.0400510132312775\n",
      "training mini-batch : 0 total loss = 0.9297984838485718\n",
      "epoch: 395 \n",
      " prediction training loss =  0.9964202077765214 validation loss =  1.0408322513103485\n",
      "training mini-batch : 0 total loss = 0.9957945346832275\n",
      "epoch: 396 \n",
      " prediction training loss =  0.9873874312952945 validation loss =  1.0359288156032562\n",
      "training mini-batch : 0 total loss = 0.8596322536468506\n",
      "epoch: 397 \n",
      " prediction training loss =  0.9948337485915736 validation loss =  1.0367693603038788\n",
      "training mini-batch : 0 total loss = 1.014528512954712\n",
      "epoch: 398 \n",
      " prediction training loss =  0.9990877258150201 validation loss =  1.038285806775093\n",
      "training mini-batch : 0 total loss = 0.9134760499000549\n",
      "epoch: 399 \n",
      " prediction training loss =  0.9867030350785506 validation loss =  1.034066766500473\n",
      "training mini-batch : 0 total loss = 0.891225278377533\n",
      "epoch: 400 \n",
      " prediction training loss =  0.9918800749276814 validation loss =  1.038111612200737\n",
      "training mini-batch : 0 total loss = 0.9569982290267944\n",
      "epoch: 401 \n",
      " prediction training loss =  0.9956866032198856 validation loss =  1.0308456420898438\n",
      "training mini-batch : 0 total loss = 1.0559872388839722\n",
      "epoch: 402 \n",
      " prediction training loss =  1.005231747501775 validation loss =  1.024090364575386\n",
      "training mini-batch : 0 total loss = 1.1091667413711548\n",
      "epoch: 403 \n",
      " prediction training loss =  1.0001837015151978 validation loss =  1.0180591940879822\n",
      "training mini-batch : 0 total loss = 1.0552228689193726\n",
      "epoch: 404 \n",
      " prediction training loss =  0.9798665674109208 validation loss =  1.008688360452652\n",
      "training mini-batch : 0 total loss = 1.0894237756729126\n",
      "epoch: 405 \n",
      " prediction training loss =  0.920729332848599 validation loss =  0.8848110884428024\n",
      "training mini-batch : 0 total loss = 0.8468248844146729\n",
      "epoch: 406 \n",
      " prediction training loss =  0.7516500981230485 validation loss =  0.5943519175052643\n",
      "training mini-batch : 0 total loss = 0.6828740835189819\n",
      "epoch: 407 \n",
      " prediction training loss =  0.5067349750744669 validation loss =  0.3752865046262741\n",
      "training mini-batch : 0 total loss = 0.4642745554447174\n",
      "epoch: 408 \n",
      " prediction training loss =  0.3935098977465379 validation loss =  0.25343866646289825\n",
      "training mini-batch : 0 total loss = 0.2727346420288086\n",
      "epoch: 409 \n",
      " prediction training loss =  0.34707791868009064 validation loss =  0.1962016001343727\n",
      "training mini-batch : 0 total loss = 0.2650526762008667\n",
      "epoch: 410 \n",
      " prediction training loss =  0.2338186298546038 validation loss =  0.16333485022187233\n",
      "training mini-batch : 0 total loss = 0.27923285961151123\n",
      "epoch: 411 \n",
      " prediction training loss =  0.22223691877565885 validation loss =  0.08460571244359016\n",
      "training mini-batch : 0 total loss = 0.21315284073352814\n",
      "epoch: 412 \n",
      " prediction training loss =  0.1577319926337192 validation loss =  0.08991489373147488\n",
      "training mini-batch : 0 total loss = 0.16864638030529022\n",
      "epoch: 413 \n",
      " prediction training loss =  0.18564529481687045 validation loss =  0.24647869542241096\n",
      "training mini-batch : 0 total loss = 0.29662230610847473\n",
      "epoch: 414 \n",
      " prediction training loss =  0.19640561350082097 validation loss =  0.05589778069406748\n",
      "training mini-batch : 0 total loss = 0.1023666262626648\n",
      "epoch: 415 \n",
      " prediction training loss =  0.13453471739041178 validation loss =  0.09202011115849018\n",
      "training mini-batch : 0 total loss = 0.15172390639781952\n",
      "epoch: 416 \n",
      " prediction training loss =  0.15741123298281118 validation loss =  0.07860362529754639\n",
      "training mini-batch : 0 total loss = 0.15876170992851257\n",
      "epoch: 417 \n",
      " prediction training loss =  0.1544666498115188 validation loss =  0.06945652700960636\n",
      "training mini-batch : 0 total loss = 0.1243773102760315\n",
      "epoch: 418 \n",
      " prediction training loss =  0.11233781787909959 validation loss =  0.050539751537144184\n",
      "training mini-batch : 0 total loss = 0.18846538662910461\n",
      "epoch: 419 \n",
      " prediction training loss =  0.11980748490283363 validation loss =  0.0350172552280128\n",
      "training mini-batch : 0 total loss = 0.07675326615571976\n",
      "epoch: 420 \n",
      " prediction training loss =  0.10119818033356416 validation loss =  0.03507921192795038\n",
      "training mini-batch : 0 total loss = 0.088780477643013\n",
      "epoch: 421 \n",
      " prediction training loss =  0.11622561435950429 validation loss =  0.03427464468404651\n",
      "training mini-batch : 0 total loss = 0.1414804607629776\n",
      "epoch: 422 \n",
      " prediction training loss =  0.11437411998447619 validation loss =  0.04607016779482365\n",
      "training mini-batch : 0 total loss = 0.15832088887691498\n",
      "epoch: 423 \n",
      " prediction training loss =  0.09900367926610143 validation loss =  0.11878042109310627\n",
      "training mini-batch : 0 total loss = 0.15008096396923065\n",
      "epoch: 424 \n",
      " prediction training loss =  0.11926204828839553 validation loss =  0.0579838240519166\n",
      "training mini-batch : 0 total loss = 0.11956363916397095\n",
      "epoch: 425 \n",
      " prediction training loss =  0.0971650855713769 validation loss =  0.02829986158758402\n",
      "training mini-batch : 0 total loss = 0.06139874830842018\n",
      "epoch: 426 \n",
      " prediction training loss =  0.08797004191499007 validation loss =  0.025593497790396214\n",
      "training mini-batch : 0 total loss = 0.0871303528547287\n",
      "epoch: 427 \n",
      " prediction training loss =  0.09487313385072507 validation loss =  0.05609041731804609\n",
      "training mini-batch : 0 total loss = 0.07997094839811325\n",
      "epoch: 428 \n",
      " prediction training loss =  0.08812080304089345 validation loss =  0.019118539057672024\n",
      "training mini-batch : 0 total loss = 0.12925668060779572\n",
      "epoch: 429 \n",
      " prediction training loss =  0.08730602558506162 validation loss =  0.018371073063462973\n",
      "training mini-batch : 0 total loss = 0.08157055079936981\n",
      "epoch: 430 \n",
      " prediction training loss =  0.07729374342843105 validation loss =  0.03083911817520857\n",
      "training mini-batch : 0 total loss = 0.0589320994913578\n",
      "epoch: 431 \n",
      " prediction training loss =  0.09411938861012459 validation loss =  0.02561495453119278\n",
      "training mini-batch : 0 total loss = 0.09347113221883774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 432 \n",
      " prediction training loss =  0.08569254569317165 validation loss =  0.01923183538019657\n",
      "training mini-batch : 0 total loss = 0.06400313228368759\n",
      "epoch: 433 \n",
      " prediction training loss =  0.09293273326597716 validation loss =  0.030611722264438868\n",
      "training mini-batch : 0 total loss = 0.14663279056549072\n",
      "epoch: 434 \n",
      " prediction training loss =  0.08650852465315868 validation loss =  0.0312616634182632\n",
      "training mini-batch : 0 total loss = 0.0837198793888092\n",
      "epoch: 435 \n",
      " prediction training loss =  0.07760282488245714 validation loss =  0.04430333524942398\n",
      "training mini-batch : 0 total loss = 0.09140577912330627\n",
      "epoch: 436 \n",
      " prediction training loss =  0.08205661373703103 validation loss =  0.025850133039057255\n",
      "training mini-batch : 0 total loss = 0.09816007316112518\n",
      "epoch: 437 \n",
      " prediction training loss =  0.0746696379623915 validation loss =  0.03741481387987733\n",
      "training mini-batch : 0 total loss = 0.07006001472473145\n",
      "epoch: 438 \n",
      " prediction training loss =  0.08112413514601557 validation loss =  0.01621064217761159\n",
      "training mini-batch : 0 total loss = 0.07312396913766861\n",
      "epoch: 439 \n",
      " prediction training loss =  0.08388341845650422 validation loss =  0.02093686256557703\n",
      "training mini-batch : 0 total loss = 0.050702907145023346\n",
      "epoch: 440 \n",
      " prediction training loss =  0.09543464313212194 validation loss =  0.05625661462545395\n",
      "training mini-batch : 0 total loss = 0.07556314766407013\n",
      "epoch: 441 \n",
      " prediction training loss =  0.1043798798008969 validation loss =  0.07125858962535858\n",
      "training mini-batch : 0 total loss = 0.12958404421806335\n",
      "epoch: 442 \n",
      " prediction training loss =  0.08448116108775139 validation loss =  0.019211257807910442\n",
      "training mini-batch : 0 total loss = 0.08803001791238785\n",
      "epoch: 443 \n",
      " prediction training loss =  0.08086486454857023 validation loss =  0.02631384413689375\n",
      "training mini-batch : 0 total loss = 0.07679112255573273\n",
      "epoch: 444 \n",
      " prediction training loss =  0.08060244274766822 validation loss =  0.05115225911140442\n",
      "training mini-batch : 0 total loss = 0.08158443123102188\n",
      "epoch: 445 \n",
      " prediction training loss =  0.10347449936364826 validation loss =  0.018091782927513123\n",
      "training mini-batch : 0 total loss = 0.08374448865652084\n",
      "epoch: 446 \n",
      " prediction training loss =  0.09327812904590055 validation loss =  0.056428142823278904\n",
      "training mini-batch : 0 total loss = 0.07978025078773499\n",
      "epoch: 447 \n",
      " prediction training loss =  0.07521590551263407 validation loss =  0.03135310206562281\n",
      "training mini-batch : 0 total loss = 0.0760977566242218\n",
      "epoch: 448 \n",
      " prediction training loss =  0.07118410224977292 validation loss =  0.025000500958412886\n",
      "training mini-batch : 0 total loss = 0.07218983769416809\n",
      "epoch: 449 \n",
      " prediction training loss =  0.07953847140858047 validation loss =  0.0821338389068842\n",
      "training mini-batch : 0 total loss = 0.1275780349969864\n",
      "epoch: 450 \n",
      " prediction training loss =  0.08781599920046956 validation loss =  0.028225754853338003\n",
      "training mini-batch : 0 total loss = 0.07526981830596924\n",
      "epoch: 451 \n",
      " prediction training loss =  0.07531718733279329 validation loss =  0.029626084957271814\n",
      "training mini-batch : 0 total loss = 0.10739045590162277\n",
      "epoch: 452 \n",
      " prediction training loss =  0.07730767091638163 validation loss =  0.04189232736825943\n",
      "training mini-batch : 0 total loss = 0.08671241998672485\n",
      "epoch: 453 \n",
      " prediction training loss =  0.07473419234156609 validation loss =  0.05737506691366434\n",
      "training mini-batch : 0 total loss = 0.11253339052200317\n",
      "epoch: 454 \n",
      " prediction training loss =  0.08271227894644988 validation loss =  0.021695151459425688\n",
      "training mini-batch : 0 total loss = 0.07856978476047516\n",
      "epoch: 455 \n",
      " prediction training loss =  0.08442156369748868 validation loss =  0.018882883014157414\n",
      "training mini-batch : 0 total loss = 0.05139129236340523\n",
      "epoch: 456 \n",
      " prediction training loss =  0.08191237892759473 validation loss =  0.05883207079023123\n",
      "training mini-batch : 0 total loss = 0.1243429034948349\n",
      "epoch: 457 \n",
      " prediction training loss =  0.07089900539109581 validation loss =  0.02158212522044778\n",
      "training mini-batch : 0 total loss = 0.049474235624074936\n",
      "epoch: 458 \n",
      " prediction training loss =  0.06138950762780089 validation loss =  0.015972128370776772\n",
      "training mini-batch : 0 total loss = 0.06225524842739105\n",
      "epoch: 459 \n",
      " prediction training loss =  0.06925406208948086 validation loss =  0.01898831152357161\n",
      "training mini-batch : 0 total loss = 0.05188669636845589\n",
      "epoch: 460 \n",
      " prediction training loss =  0.08850659626094919 validation loss =  0.0704717319458723\n",
      "training mini-batch : 0 total loss = 0.09678055346012115\n",
      "epoch: 461 \n",
      " prediction training loss =  0.06611520698980282 validation loss =  0.019204183714464307\n",
      "training mini-batch : 0 total loss = 0.06799790263175964\n",
      "epoch: 462 \n",
      " prediction training loss =  0.07404843658993118 validation loss =  0.026292589493095875\n",
      "training mini-batch : 0 total loss = 0.04506854712963104\n",
      "epoch: 463 \n",
      " prediction training loss =  0.06561457816707461 validation loss =  0.016149354400113225\n",
      "training mini-batch : 0 total loss = 0.05970350652933121\n",
      "epoch: 464 \n",
      " prediction training loss =  0.07561186014821655 validation loss =  0.03734590392559767\n",
      "training mini-batch : 0 total loss = 0.10377287864685059\n",
      "epoch: 465 \n",
      " prediction training loss =  0.06784301878590333 validation loss =  0.024204402696341276\n",
      "training mini-batch : 0 total loss = 0.07662370800971985\n",
      "epoch: 466 \n",
      " prediction training loss =  0.05942715959329354 validation loss =  0.031169361900538206\n",
      "training mini-batch : 0 total loss = 0.08256058394908905\n",
      "epoch: 467 \n",
      " prediction training loss =  0.07480078916016378 validation loss =  0.061314428225159645\n",
      "training mini-batch : 0 total loss = 0.09561055153608322\n",
      "epoch: 468 \n",
      " prediction training loss =  0.07581590625800584 validation loss =  0.01734086312353611\n",
      "training mini-batch : 0 total loss = 0.06077292934060097\n",
      "epoch: 469 \n",
      " prediction training loss =  0.05928745375652062 validation loss =  0.027566558215767145\n",
      "training mini-batch : 0 total loss = 0.062298212200403214\n",
      "epoch: 470 \n",
      " prediction training loss =  0.06221817375013703 validation loss =  0.026111128740012646\n",
      "training mini-batch : 0 total loss = 0.06726843863725662\n",
      "epoch: 471 \n",
      " prediction training loss =  0.07758187718297306 validation loss =  0.02318049594759941\n",
      "training mini-batch : 0 total loss = 0.07068857550621033\n",
      "epoch: 472 \n",
      " prediction training loss =  0.07366449444701798 validation loss =  0.03555263811722398\n",
      "training mini-batch : 0 total loss = 0.05239498242735863\n",
      "epoch: 473 \n",
      " prediction training loss =  0.07414772988934266 validation loss =  0.03283451031893492\n",
      "training mini-batch : 0 total loss = 0.08385679870843887\n",
      "epoch: 474 \n",
      " prediction training loss =  0.07698912546038628 validation loss =  0.06171284709125757\n",
      "training mini-batch : 0 total loss = 0.04451727867126465\n",
      "epoch: 475 \n",
      " prediction training loss =  0.0788427316828778 validation loss =  0.035824548453092575\n",
      "training mini-batch : 0 total loss = 0.0950179398059845\n",
      "epoch: 476 \n",
      " prediction training loss =  0.0629956123272055 validation loss =  0.0308948983438313\n",
      "training mini-batch : 0 total loss = 0.048919521272182465\n",
      "epoch: 477 \n",
      " prediction training loss =  0.07947176341947756 validation loss =  0.030001433100551367\n",
      "training mini-batch : 0 total loss = 0.05861546844244003\n",
      "epoch: 478 \n",
      " prediction training loss =  0.06123637721726769 validation loss =  0.03318420983850956\n",
      "training mini-batch : 0 total loss = 0.06133356690406799\n",
      "epoch: 479 \n",
      " prediction training loss =  0.0709704523415942 validation loss =  0.015702253207564354\n",
      "training mini-batch : 0 total loss = 0.04885951802134514\n",
      "epoch: 480 \n",
      " prediction training loss =  0.06414122585403292 validation loss =  0.026449274737387896\n",
      "training mini-batch : 0 total loss = 0.06696831434965134\n",
      "epoch: 481 \n",
      " prediction training loss =  0.07733839201299768 validation loss =  0.02411686349660158\n",
      "training mini-batch : 0 total loss = 0.06250827014446259\n",
      "epoch: 482 \n",
      " prediction training loss =  0.07086008689121197 validation loss =  0.033380393870174885\n",
      "training mini-batch : 0 total loss = 0.0446593277156353\n",
      "epoch: 483 \n",
      " prediction training loss =  0.06382501341010395 validation loss =  0.019984067417681217\n",
      "training mini-batch : 0 total loss = 0.05907593294978142\n",
      "epoch: 484 \n",
      " prediction training loss =  0.0605054572224617 validation loss =  0.015473636332899332\n",
      "training mini-batch : 0 total loss = 0.04898405075073242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 485 \n",
      " prediction training loss =  0.06426294893026352 validation loss =  0.02065833890810609\n",
      "training mini-batch : 0 total loss = 0.05369577929377556\n",
      "epoch: 486 \n",
      " prediction training loss =  0.05735582779896887 validation loss =  0.020554442889988422\n",
      "training mini-batch : 0 total loss = 0.04175964370369911\n",
      "epoch: 487 \n",
      " prediction training loss =  0.07213417813181877 validation loss =  0.03337884182110429\n",
      "training mini-batch : 0 total loss = 0.04000739008188248\n",
      "epoch: 488 \n",
      " prediction training loss =  0.06868898486228366 validation loss =  0.028285215608775616\n",
      "training mini-batch : 0 total loss = 0.054474055767059326\n",
      "epoch: 489 \n",
      " prediction training loss =  0.05528305921899645 validation loss =  0.0279597588814795\n",
      "training mini-batch : 0 total loss = 0.026222234591841698\n",
      "epoch: 490 \n",
      " prediction training loss =  0.07405550277938969 validation loss =  0.021575204096734524\n",
      "training mini-batch : 0 total loss = 0.05132108926773071\n",
      "epoch: 491 \n",
      " prediction training loss =  0.06454506320388693 validation loss =  0.014507148414850235\n",
      "training mini-batch : 0 total loss = 0.03841884806752205\n",
      "epoch: 492 \n",
      " prediction training loss =  0.05906264621176218 validation loss =  0.014312139013782144\n",
      "training mini-batch : 0 total loss = 0.06293686479330063\n",
      "epoch: 493 \n",
      " prediction training loss =  0.07668951251789143 validation loss =  0.0617157518863678\n",
      "training mini-batch : 0 total loss = 0.04929955303668976\n",
      "epoch: 494 \n",
      " prediction training loss =  0.06778936931177189 validation loss =  0.020948288962244987\n",
      "training mini-batch : 0 total loss = 0.07836717367172241\n",
      "epoch: 495 \n",
      " prediction training loss =  0.05806543854506392 validation loss =  0.03479872411116958\n",
      "training mini-batch : 0 total loss = 0.12750399112701416\n",
      "epoch: 496 \n",
      " prediction training loss =  0.06833644848512976 validation loss =  0.016303815646097064\n",
      "training mini-batch : 0 total loss = 0.06555790454149246\n",
      "epoch: 497 \n",
      " prediction training loss =  0.05217759997436875 validation loss =  0.020786555483937263\n",
      "training mini-batch : 0 total loss = 0.046042606234550476\n",
      "epoch: 498 \n",
      " prediction training loss =  0.05261226566998582 validation loss =  0.013969634426757693\n",
      "training mini-batch : 0 total loss = 0.07384516298770905\n",
      "epoch: 499 \n",
      " prediction training loss =  0.059705148205945364 validation loss =  0.032003194093704224\n",
      "training mini-batch : 0 total loss = 0.061490681022405624\n",
      "epoch: 500 \n",
      " prediction training loss =  0.05409603262026059 validation loss =  0.017679258948192\n"
     ]
    }
   ],
   "source": [
    "for k in range(epoch):\n",
    "\n",
    "    if k and k % save_model_every == 0:\n",
    "        save_model(folder, k, n_base, base_hidden, grid, sub_hidden, dropout, lambda1, lambda2, model, optimizer)\n",
    "\n",
    "    pred_loss_train = []\n",
    "    total_loss_train = []\n",
    "    loss_valid = []\n",
    "    dataLoader.shuffle()\n",
    "    # set model training state\n",
    "    model.train()\n",
    "\n",
    "    for i, (x, y) in enumerate(dataLoader.get_train_batch()):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model.forward(x)\n",
    "        loss_pred = compute_loss(out, y)\n",
    "        loss = loss_pred + model.R1(l1_k) + model.R2(l2_pairs)\n",
    "        # record training loss history\n",
    "        total_loss_train.append(loss.item())\n",
    "        pred_loss_train.append(loss_pred.item())\n",
    "\n",
    "        # update parameters using backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\"training mini-batch :\", i,\n",
    "                  \"total loss =\", total_loss_train[-1])\n",
    "\n",
    "    total_loss_train_history.append(np.mean(total_loss_train))\n",
    "    pred_loss_train_history.append(np.mean(pred_loss_train))\n",
    "\n",
    "    # model evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for x, y in dataLoader.get_valid_batch():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            valid_y = model.forward(x)\n",
    "            valid_loss = compute_loss(valid_y, y)\n",
    "            # print(\"valid - check out: \", check_tensor([valid_loss]))\n",
    "            loss_valid.append(valid_loss.item())\n",
    "\n",
    "    if np.mean(loss_valid) < min_valid_loss:\n",
    "        save_model(folder, \"best\", n_base, base_hidden, grid, sub_hidden, dropout, lambda1, lambda2, model, optimizer)\n",
    "        min_valid_loss = np.mean(loss_valid)\n",
    "\n",
    "    loss_valid_history.append(np.mean(loss_valid))\n",
    "\n",
    "    print(\"epoch:\", k+1, \"\\n\",\n",
    "          \"prediction training loss = \", pred_loss_train_history[-1],\n",
    "          \"validation loss = \", loss_valid_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242dd67",
   "metadata": {},
   "source": [
    "Make a loss plot after training finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d62cf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(range(1, epoch+1)), total_loss_train_history, label='train_total')\n",
    "plt.plot(list(range(1, epoch+1)), pred_loss_train_history, label='train_pred')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "# plt.savefig('loss_plot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603e3a6",
   "metadata": {},
   "source": [
    "Let's plot the learned bases using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee342b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bases(device=torch.device(\"cpu\"), folder=\"train/\"):\n",
    "    ck = folder + \"best_checkpoint.pth\"\n",
    "    model, t = load_model(ck, device)\n",
    "    T = torch.tensor(t).to(device)\n",
    "    t = np.array(t)\n",
    "    base_folder = folder + \"bases/\"\n",
    "\n",
    "    if not Path(base_folder).is_dir():\n",
    "        Path(base_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    bases = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, basis in enumerate(model.BL):\n",
    "            T = T.unsqueeze(dim=-1)\n",
    "            y = np.squeeze(basis(T).squeeze(dim=-1).detach().cpu().numpy())\n",
    "            y_sq = y ** 2\n",
    "            l2_norm = np.sqrt(np.sum((y_sq[:-1] + y_sq[1:]) * (t[1:] - t[:-1])) / 2)\n",
    "            bases.append(y / l2_norm)\n",
    "\n",
    "    B = len(bases)\n",
    "    fig, axs = plt.subplots(1, B)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    for i in range(B):        \n",
    "        axs[i].plot(t, bases[i], linewidth=3.5, label=\"basis\"+str(i+1))\n",
    "        axs[i].legend()\n",
    "    \n",
    "    return bases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9537c10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bases = plot_bases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4735",
   "metadata": {},
   "source": [
    "We can plot a linear combination of these two learned bases and see what it looks compared to the true one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78791329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(t, y, color, label):\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.plot(grid, phi5(np.array(grid)), linewidth=3.5, color=color, label=label)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40607256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_b = [np.array(b) for b in bases]\n",
    "lin = np_b[1] - np_b[0]\n",
    "plot(grid, lin, \"orangered\", \"learned phi2 - phi1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b646673",
   "metadata": {},
   "source": [
    "We can also plot the true signal $\\phi_5$ in Case 3. We can see that two bases learned the truth successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "276a99f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi5 = _phi(5)\n",
    "plot(grid, phi5(np.array(grid)), \"green\", \"true phis5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ec934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b446d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
